<?xml version="1.0" encoding="UTF-8"?>
<bank>
  <topic>PDE18 - Brain Dump (google.practicetest.professional-data-engineer.pdf.2024-feb-10.by.archer.81q.vce)</topic>
  <!-- STRUCTURE DEFINITION:
  </entry>
  <entry>
    <question>XXX</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>XXX</detail>
        </option>
      </options>
    </answer>
  </entry>
  -->
  <entry>
    <question>You want to use a database of information about tissue samples to classify future tissue samples as either normal or mutated. You are evaluating an unsupervised anomaly detection method for classifying the tissue samples. Which two characteristic support this method? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>There are very few occurrences of mutations relative to normal samples.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>There are roughly equal occurrences of both normal and mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You expect future mutations to have different features from the mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You expect future mutations to have similar features to the mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You already have labels for which samples are mutated and which are normal in the database.</detail>
        </option>
      </options>
    </answer>Answer:  AD
  </entry>
 
  <entry>
    <question>You have spent a few days loading data from comma-separated values (CSV) files into the Google BigQuery table CLICK_STREAM. The column DT stores the epoch time of click events. For convenience, you chose a simple schema where every field is treated as the STRING type. Now, you want to compute web session durations of users who visit your site, and you want to change its data type to the TIMESTAMP. You want to minimize the migration effort without making future queries computationally expensive. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Delete the table CLICK_STREAM, and then re-create it such that the column DT is of the TIMESTAMP typ</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reload the data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a column TS of the TIMESTAMP type to the table CLICK_STREAM, and populate the numeric values from the column TS for each ro</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reference the column TS instead of the column DT from now on.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a view CLICK_STREAM_V, where strings from the column DT are cast into TIMESTAMP value</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reference the view CLICK_STREAM_V instead of the table CLICK_STREAM from now on.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add two columns to the table CLICK STREAM: TS of the TIMESTAMP type and IS_NEW of the BOOLEAN typ</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reload all data in append mod
   </entry>
  <entry>
    <question></detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>For each appended row, set the value of IS_NEW to tru</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>For future queries, reference the column TS instead of the column DT, with the WHERE clause ensuring that the value of IS_NEW must be true.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Construct a query to return every row of the table CLICK_STREAM, while using the built-in function to cast strings from the column DT into TIMESTAMP value L. Run the query into a destination table NEW_CLICK_STREAM, in which the column TS is the TIMESTAMP typ
M. Reference the table NEW_CLICK_STREAM instead of the table CLICK_STREAM from now o
N. In the future, new data is loaded into the table NEW_CLICK_STREAM.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>You work for a car manufacturer and have set up a data pipeline using Google Cloud Pub/Sub to capture anomalous sensor events. You are using a push subscription in Cloud Pub/Sub that calls a custom HTTPS endpoint that you have created to take action of these anomalous events as they occur. Your custom HTTPS endpoint keeps getting an inordinate amount of duplicate messages. What is the most likely cause of these duplicate messages?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The message body for the sensor event is too large.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Your custom endpoint has an out-of-date SSL certificate.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The Cloud Pub/Sub topic has too many messages published to it.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Your custom endpoint is not acknowledging messages within the acknowledgement deadline.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>Your company is performing data preprocessing for a learning algorithm in Google Cloud Dataflow. Numerous data logs are being are being generated during this step, and the team wants to analyze them. Due to the dynamic nature of the campaign, the data is growing exponentially every hour.
The data scientists have written the following code to read the data for a new key features in the logs. BigQueryIO.Read
.named(“ReadLogData”)
.from(“clouddataflow-readonly:samples.log_data”)
You want to improve the performance of this data read. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Specify the TableReference object in the code.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use .fromQuery operation to read specific fields from the table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use of both the Google BigQuery TableSchema and TableFieldSchema classes.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Call a transform that returns TableRow objects, where each element in the PCollexction represents a single row in the table.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>Your company uses a proprietary system to send inventory data every 6 hours to a data ingestion service in the cloud. Transmitted data includes a payload of several fields and the timestamp of the transmission. If there are any concerns about a transmission, the system re-transmits the data. How should you deduplicate the data most efficiency?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Assign global unique identifiers (GUID) to each data entry.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Compute the hash value of each data entry, and compare it with all historical data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store each data entry as the primary key in a separate database and apply an index.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Maintain a database table to store the hash value and other metadata for each data entry.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>Your company built a TensorFlow neural-network model with a large number of neurons and layers. The model fits well for the training data. However, when tested against new data, it performs poorly. What method can you employ to address this?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Threading</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Serialization</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dropout Methods</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dimensionality Reduction</detail>
        </option>
      </options>
    </answer>Answer:  C

  </entry>

  <entry>
    <question>You are designing the database schema for a machine learning-based food ordering service that will predict what users want to eat. Here is some of the information you need to store:
The user profile: What the user likes and doesn’t like to eat
The user account information: Name, address, preferred meal times
The order information: When orders are made, from where, to whom
The database will be used to store all the transactional data of the product. You want to optimize the data schema. Which Google Cloud Platform product should you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud SQL</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Bigtable</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Datastore</detail>
        </option>
      </options>
    </answer>Answer:  A
   


Which of these is NOT a way to customize the software on Dataproc cluster instances?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Set initialization actions</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Modify configuration files using cluster properties</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Configure the cluster using Cloud Deployment Manager</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Log into the master node and make changes from there</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>Which action can a Cloud Dataproc Viewer perform?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Submit a job.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a cluster.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Delete a cluster.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>List the jobs.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>Which Java SDK class can you use to run your Dataflow programs locally?
 A. LocalRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>DirectPipelineRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>MachineRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>LocalPipelineRunner</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>

  <entry>
    <question>What is the general recommendation when designing your row keys for a Cloud Bigtable schema?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Include multiple time series values within the row key</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep the row keep as an 8 bit integer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep your row key reasonably short</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep your row key as long as the field permits</detail>
        </option>
      </options>
    </answer>Answer:  C

Suppose you have a dataset of images that are each labeled as to whether or not they contain a human face. To create a neural network that recognizes human faces in images using this labeled dataset, what approach would likely be the most effective?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use K-means Clustering to detect faces in the pixels.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use feature engineering to add features for eyes, noses, and mouths to the input data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use deep learning by creating a neural network with multiple hidden layers to automatically detect features of faces.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Build a neural network with an input layer of pixels, a hidden layer, and an output layer with two categories.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question></detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>you want to create a machine learning model that predicts the price of a particular stock based on its recent price history, what type of estimator should you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Unsupervised learning</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Regressor</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Classifier</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Clustering estimator</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>

  <entry>
    <question>Which of these operations can you perform from the BigQuery Web UI?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Upload a file in SQL format.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load data with nested and repeated fields.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload a 20 MB file.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload multiple files using a wildcard.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>When using Cloud Dataproc clusters, you can access the YARN web interface by configuring a browser to connect through a proxy.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>HTTPS</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>VPN</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>SOCKS</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>HTTP</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>You decided to use Cloud Datastore to ingest vehicle telemetry data in real time. You want to build a storage system that will account for the long-term data growth, while keeping the costs low. You also want to create snapshots of the data periodically, so that you can make a point-in-time (PIT) recovery, or clone a copy of the data for Cloud Datastore in a different environment. You want to archive these snapshots for a long time. Which two methods can accomplish this? Choose 2 answers.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use managed export, and store the data in a Cloud Storage bucket using Nearline or Coldline class.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use managed exportm, and then import to Cloud Datastore in a separate project under a unique namespace reserved for that export.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use managed export, and then import the data into a BigQuery table created just for that export, and delete temporary export files.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write an application that uses Cloud Datastore client libraries to read all the entitie</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Treat each entity as a BigQuery table row via BigQuery streaming inser</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Assign an export timestamp for each export, and attach it as an extra column for each ro</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Make sure that the BigQuery table is partitioned using the export timestamp column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write an application that uses Cloud Datastore client libraries to read all the entitie</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Format the exported data into a JSON fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Apply compression before storing the data in Cloud Source Repositories.</detail>
        </option>
      </options>
    </answer>Answer:  CE
  </entry>
  <entry>
    <question></question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>online retailer has built their current application on Google App Engine. A new initiative at the company mandates that they extend their application to allow their customers to transact directly via the application.
They need to manage their shopping transactions and analyze combined data from multiple datasets using a business intelligence (BI) tool. They want to use only a single database for this purpose. Which Google Cloud database should they choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud SQL</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud BigTable</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Datastore</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>You need to copy millions of sensitive patient records from a relational database to BigQuery. The total size of the database is 10 TB. You need to design a solution that is secure and time-efficient. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Export the records from the database as an Avro fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload the file to GCS using gsutil, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the records from the database as an Avro fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Copy the file onto a Transfer Appliance and send it to Google, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console. E. Export the records from the database into a CSV fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a public URL for the CSV file, and then use Storage Transfer Service to move the file to Cloud Storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load the CSV file into BigQuery using the BigQuery web UI in the GCP Console.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the records from the database as an Avro fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a public URL for the Avro file, and thenuse Storage Transfer Service to move the file to Cloud Storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>You are building a teal-lime prediction engine that streams files, which may contain Pll (personal identifiable information) data, into Cloud Storage and eventually into BigQuery You want to ensure that the sensitive data is masked but still maintains referential Integrity, because names and emails are often used as join keys How should you use the Cloud Data Loss Prevention API (DLP API) to ensure that the Pll data is not accessible by unauthorized individuals?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a pseudonym by replacing the Pll data with cryptogenic tokens, and store the non-tokenized data in a locked-down button.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Redact all Pll data, and store a version of the unredacted data in a locked-down bucket</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Scan every table in BigQuery, and mask the data it finds that has Pll</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a pseudonym by replacing Pll data with a cryptographic format-preserving token</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>You work for a shipping company that uses handheld scanners to read shipping labels. Your company has strict data privacy standards that require scanners to only transmit recipients’ personally identifiable information (PII) to analytics systems, which violates user privacy rules. You want to quickly build a scalable solution using cloud-native managed services to prevent exposure of PII to the analytics systems. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create an authorized view in BigQuery to restrict access to tables with sensitive data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install a third-party data validation tool on Compute Engine virtual machines to check the incoming datafor sensitive information.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Stackdriver logging to analyze the data passed through the total pipeline to identify transactions that may contain sensitive information.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Build a Cloud Function that reads the topics and makes a call to the Cloud Data Loss Prevention AP</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the tagging and confidence levels to either pass or quarantine the data in a bucket for review.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>2

You have historical data covering the last three years in BigQuery and a data pipeline that delivers new data to BigQuery daily. You have noticed that when the Data Science team runs a query filtered on a date column and limited to 30–90 days of data, the query scans the entire table. You also noticed that your bill is increasing more quickly than you expected. You want to resolve the issue as cost-effectively as possible while maintaining the ability to conduct SQL queries. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Re-create the tables using DD</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Partition the tables by a column containing a TIMESTAMP or DATEType.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Recommend that the Data Science team export the table to a CSV file on Cloud Storage and use Cloud Datalab to explore the data by reading the files directly.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Modify your pipeline to maintain the last 30–90 days of data in one table and the longer history in a different table to minimize full table scans over the entire history.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write an Apache Beam pipeline that creates a BigQuery table per da</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Recommend that the Data Science team use wildcards on the table name suffixes to select the data they need.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>7

You are a head of BI at a large enterprise company with multiple business units that each have different priorities and budgets. You use on-demand pricing for BigQuery with a quota of 2K concurrent on-demand slots per project. Users at your organization sometimes don’t get slots to execute their query and you need to correct this. You’d like to avoid introducing new projects to your account.
What should you do?
  
</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Convert your batch BQ queries into interactive BQ queries.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create an additional project to overcome the 2K on-demand per-project quota.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Switch to flat-rate pricing and establish a hierarchical priority model for your projects.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the amount of concurrent slots per project at the Quotas page at the Cloud Console.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>2
</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>aerospace company uses a proprietary data format to store its night data. You need to connect this new data source to BigQuery and stream the data into BigQuery. You want to efficiency import the data into BigQuery where consuming as few resources as possible. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use a standard Dataflow pipeline to store the raw data in BigQuery and then transform the format later when the data is used.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write a shell script that triggers a Cloud Function that performs periodic ETL batch jobs on the new data source</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Apache Hive to write a Dataproc job that streams the data into BigQuery in CSV format</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use an Apache Beam custom connector to write a Dataflow pipeline that streams the data into BigQuery in Avro format</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>4

A live TV show asks viewers to cast votes using their mobile phones. The event generates a large volume of data during a 3 minute period. You are in charge of the Voting restructure* and must ensure that the platform can handle the load and Hal all votes are processed. You must display partial results write voting is open. After voting doses you need to count the votes exactly once white optimizing cost. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a Memorystore instance with a high availability (HA) configuration</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write votes to a Pub Sub tope and have Cloud Functions subscribe to it and write voles to BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write votes to a Pub/Sub tope and toad into both Bigtable and BigQuery via a Dataflow pipeline Query Bigtable for real-time results and BigQuery for later analysis Shutdown the Bigtable instance when voting concludesD Create a Cloud SQL for PostgreSQL database with high availability (HA) configuration and multiple read replicas</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>9

You are selecting services to write and transform JSON messages from Cloud Pub/Sub to BigQuery for a data pipeline on Google Cloud. You want to minimize service costs. You also want to monitor and accommodate input data volume that will vary in size with minimal manual intervention. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataproc to run your transformation</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Monitor CPU utilization for the cluste</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Resize the number of worker nodes in your cluster via the command line.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataproc to run your transformation</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the diagnose command to generate an operational output archiv</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Locate the bottleneck and adjust cluster resources.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to run your transformation</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Monitor the job system lag with Stackdrive</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the default autoscaling setting for worker instances.
  </detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to run your transformation</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Monitor the total execution time for a sampling of job
L. Configure the job to use non-default Compute Engine machine types when needed.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>4

You have a requirement to insert minute-resolution data from 50,000 sensors into a BigQuery table. You expect significant growth in data volume and need the data to be available within 1 minute of ingestion for real-time analysis of aggregated trends. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use bq load to load a batch of sensor data every 60 seconds.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a Cloud Dataflow pipeline to stream data into the BigQuery table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the INSERT statement to insert a batch of data every 60 seconds.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the MERGE statement to apply updates in batch every 60 seconds.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>9

You are migrating an application that tracks library books and information about each book, such as author or year published, from an on-premises data warehouse to BigQuery In your current relational database, the author information is kept in a separate table and joined to the book information on a common key Based on Google's recommended practice for schema design, how would you structure the data to ensure optimal speed of queries about the author of each book that has been borrowed?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Keep the schema the same, maintain the different tables for the book and each of the attributes, and query as you are doing today</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a table that is wide and includes a column for each attribute, including the author's first name, last name, date of birth, etc</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a table that includes information about the books and authors, but nest the author fields inside the author column</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep the schema the same, create a view that joins all of the tables, and always query the view</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>

  <entry>
    <question>5

Your neural network model is taking days to train. You want to increase the training speed. What can you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Subsample your test dataset.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Subsample your training dataset.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the number of input features to your model.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the number of layers in your neural network.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>7

You set up a streaming data insert into a Redis cluster via a Kafka cluster. Both clusters are running on Compute Engine instances. You need to encrypt data at rest with encryption keys that you can create, rotate, and destroy as needed. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a dedicated service account, and use encryption at rest to reference your data stored in your Compute Engine cluster instances as part of your API service calls.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create encryption keys in Cloud Key Management Servic</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use those keys to encrypt your data in all of the Compute Engine cluster instances.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create encryption keys locall</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload your encryption keys to Cloud Key Management Servic</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use those keys to encrypt your data in all of the Compute Engine cluster instances.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create encryption keys in Cloud Key Management Servic</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reference those keys in your API service calls when accessing the data in your Compute Engine cluster instances.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>0

You want to migrate an on-premises Hadoop system to Cloud Dataproc. Hive is the primary tool in use, and the data format is Optimized Row Columnar (ORC). All ORC files have been successfully copied to a Cloud Storage bucket. You need to replicate some data to the cluster’s local Hadoop Distributed File System
(HDFS) to maximize performance. What are two ways to start using Hive in Cloud Dataproc? (Choose two.)
</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDF</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Mount the Hive tables locally.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluste</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Mount the Hive tables locally.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluste F. Then run the Hadoop utility to copy them do HDF</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Mount the Hive tables from HDFS.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Leverage Cloud Storage connector for Hadoop to mount the ORC files as external Hive table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Replicate external Hive tables to the native ones.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load the ORC files into BigQuer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Leverage BigQuery connector for Hadoop to mount the BigQuery tables as external Hive table
L. Replicate external Hive tables to the native ones.</detail>
        </option>
      </options>
    </answer>Answer:  BC
  </entry>

  <entry>
    <question>7
You are implementing security best practices on your data pipeline. Currently, you are manually executing
jobs as the Project Owner. You want to automate these jobs by taking nightly batch files containing non-public information from Google Cloud Storage, processing them with a Spark Scala job on a Google Cloud Dataproc cluster, and depositing the results into Google BigQuery.
How should you securely run this workload?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Restrict the Google Cloud Storage bucket so only you can see the files</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Grant the Project Owner role to a service account, and run the job with it</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a service account with the ability to read the batch files and to write to BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a user account with the Project Viewer role on the Cloud Dataproc cluster to read the batch files and write to BigQuery</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>8

A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to implement a change that would improve query performance in BigQuery. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Implement clustering in BigQuery on the ingest date column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Implement clustering in BigQuery on the package-tracking ID column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Tier older data onto Cloud Storage files, and leverage extended tables.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Re-create the table using data partitioning on the package delivery date.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>8

You need to choose a database to store time series CPU and memory usage for millions of computers. You need to store this data in one-second interval samples. Analysts will be performing real-time, ad hoc analytics against the database. You want to avoid being charged for every query executed and ensure that the schema design will allow for future growth of the dataset. Which database and data model should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a table in BigQuery, and append the new samples for CPU and memory to the table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a wide table in BigQuery, create a column for the sample value at each second, and update the row with the interval for each second</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a narrow table in Cloud Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a wide table in Cloud Bigtable with a row key that combines the computer identifier with the sample time at each minute, and combine the values for each second as column data.</detail>
        </option>
      </options>
    </answer>Answer:  C


You are designing storage for two relational tables that are part of a 10-TB database on Google Cloud. You want to support transactions that scale horizontally. You also want to optimize data for range queries on nonkey columns. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud SQL for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add secondary indexes to support query patterns.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud SQL for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to transform data to support query patterns. E. Use Cloud Spanner for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add secondary indexes to support query patterns.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Spanner for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to transform data to support query patterns.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>4

You are migrating your data warehouse to Google Cloud and decommissioning your on-premises data center Because this is a priority for your company, you know that bandwidth will be made available for the initial data load to the cloud. The files being transferred are not large in number, but each file is 90 GB Additionally, you want your transactional systems to continually update the warehouse on Google Cloud in real time What tools should you use to migrate the data and ensure that it continues to write to your warehouse?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Storage Transfer Service for the migration, Pub/Sub and Cloud Data Fusion for the real-time updates</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>BigQuery Data Transfer Service for the migration, Pub/Sub and Dataproc for the real-time updates</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>gsutil for the migration; Pub/Sub and Dataflow for the real-time updates</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>gsutil for both the migration and the real-time updates</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>

</bank>