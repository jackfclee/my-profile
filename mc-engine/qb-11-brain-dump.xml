<?xml version="1.0" encoding="UTF-8"?>
<bank>
  <topic>PDE11 - Brain Dump (google.ensurepass.professional-data-engineer.free.draindumps.2024-jan-06.by.gary.165q.vce)</topic>
  <!-- STRUCTURE DEFINITION:
  </entry>
  <entry>
    <question>XXX</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>XXX</detail>
        </option>
      </options>
    </answer>
  </entry>
  -->

  <entry>
    <question>Your startup has never implemented a formal security policy. Currently, everyone in the company has access to the datasets stored in Google BigQuery. Teams have freedom to use the service as they see fit, and they have not documented their use cases. You have been asked to secure the data warehouse. You need to discover what everyone is doing. What should you do first?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Google Stackdriver Audit Logs to review data access.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Get the identity and access management IIAM) policy of each table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Stackdriver Monitoring to see the usage of BigQuery query slots.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the Google Cloud Billing API to see what account the warehouse is being billed to.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>You want to process payment transactions in a point-of-sale application that will run on Google Cloud Platform. Your user base could grow exponentially, but you do not want to manage infrastructure scaling.
Which Google database service should you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Cloud SQL</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Bigtable</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Datastore</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>You are working on a sensitive project involving private user data. You have set up a project on Google Cloud Platform to house your work internally. An external consultant is going to assist with coding a complex transformation in a Google Cloud Dataflow pipeline for your project. How should you maintain users’ privacy?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Grant the consultant the Viewer role on the project.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Grant the consultant the Cloud Dataflow Developer role on the project.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a service account and allow the consultant to log on with it.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create an anonymized sample of the data for the consultant to work with in a different project.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>Your company built a TensorFlow neutral-network model with a large number of neurons and layers. The model fits well for the training data. However, when tested against new data, it performs poorly. What method can you employ to address this?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Threading</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Serialization</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dropout Methods</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dimensionality Reduction</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>

  <entry>
    <question>Your company is loading comma-separated values (CSV) files into Google BigQuery. The data is fully imported successfully; however, the imported data is not matching byte-to-byte to the source file. What is the most likely cause of this problem?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The CSV data loaded in BigQuery is not flagged as CSV.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The CSV data has invalid rows that were skipped on import.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The CSV data loaded in BigQuery is not using BigQuery’s default encoding.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The CSV data has not gone through an ETL phase before loading into BigQuery.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>Which of these are examples of a value in a sparse vector? (Select 2 answers.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>[0, 5, 0, 0, 0, 0]</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>[0, 0, 0, 1, 0, 0, 1]</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>[0, 1]</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>[1, 0, 0, 0, 0, 0, 0]</detail>
        </option>
      </options>
    </answer>Answer:  CD
  </entry>

  <entry>
    <question>Which of these sources can you not load data into BigQuery from?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>File upload</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Google Drive</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Google Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Google Cloud SQL</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>When you store data in Cloud Bigtable, what is the recommended minimum amount of stored data?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>500 TB</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>1 GB</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>1 TB</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>500 GB</detail>
        </option>
      </options>
    </answer>Answer:  C
   </entry>

  <entry>
    <question>Which of the following IAM roles does your Compute Engine account require to be able to run pipeline jobs?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>dataflow.worker</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>dataflow.compute</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>dataflow.developer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>dataflow.viewer</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>

  <entry>
    <question>Which of the following statements about the Wide & Deep Learning model are true? (Select 2 answers.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The wide model is used for memorization, while the deep model is used for generalization.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>A good use for the wide and deep model is a recommender system.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The wide model is used for generalization, while the deep model is used for memorization.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>A good use for the wide and deep model is a small-scale linear regression problem.</detail>
        </option>
      </options>
    </answer>Answer:  AB
  </entry>
  <entry>
    <question>When creating a new Cloud Dataproc cluster with the projects.regions.clusters.create operation, these four values are required: project, region, name, and .</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>zone</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>node</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>label</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>type</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>Which Cloud Dataflow / Beam feature should you use to aggregate data in an unbounded data source every hour based on the time when the data entered the pipeline?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>An hourly watermark</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>An event time trigger</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The with Allowed Lateness method</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>A processing time trigger</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>

  <entry>
    <question>Which SQL keyword can be used to reduce the number of columns processed by BigQuery?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>BETWEEN</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>WHERE</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>SELECT</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>LIMIT</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>

  <entry>
    <question>Your team is responsible for developing and maintaining ETLs in your company. One of your Dataflow jobs is failing because of some errors in the input data, and you need to improve reliability of the pipeline (incl. being able to reprocess all failing data). What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Add a filtering step to skip these types of errors in the future, extract erroneous rows from logs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a try... catch block to your DoFn that transforms the data, extract erroneous rows from logs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a try... catch block to your DoFn that transforms the data, write erroneous rows to PubSub directly from the DoFn.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a try... catch block to your DoFn that transforms the data, use a sideOutput to create a PCollection that can be stored to PubSub later.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>Your company maintains a hybrid deployment with GCP, where analytics are performed on your anonymized customer data. The data are imported to Cloud Storage from your data center through parallel uploads to a data transfer server running on GCP. Management informs you that the daily transfers take too long and have asked you to fix the problem. You want to maximize transfer speeds. Which action should you take?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Increase the CPU size on your server.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the size of the Google Persistent Disk on your server.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase your network bandwidth from your datacenter to GCP.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase your network bandwidth from Compute Engine to Cloud Storage.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>

  <entry>
    <question>You have some data, which is shown in the graphic below. The two dimensions are X and Y, and the shade of each dot represents what class it is. You want to classify this data accurately using a linear algorithm.
To do this you need to add a synthetic feature. What should the value of that feature be?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>X^2+Y^2</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>X^2</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Y^2</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>cos(X)</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>Your company has a hybrid cloud initiative. You have a complex data pipeline that moves data between cloud provider services and leverages services from each of the cloud providers. Which cloud-native service should you use to orchestrate the entire pipeline?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Cloud Dataflow</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Composer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Dataprep</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Dataproc</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>After migrating ETL jobs to run on BigQuery, you need to verify that the output of the migrated jobs is the same as the output of the original. You’ve loaded a table containing the output of the original job and want to compare the contents with output from the migrated job to show that they are identical. The tables do not contain a primary key column that would enable you to join them together for comparison.
What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Select random samples from the tables using the RAND() function and compare the samples.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Select random samples from the tables using the HASH() function and compare the samples.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a Dataproc cluster and the BigQuery Hadoop connector to read the data from each table and calculate a hash from non-timestamp columns of the table after sortin</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Compare the hashes of each table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create stratified random samples using the OVER() function and compare equivalent samples from each table.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>You use a dataset in BigQuery for analysis. You want to provide third-party companies with access to the same dataset. You need to keep the costs of data sharing low and ensure that the data is current. Which solution should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create an authorized view on the BigQuery table to control data access, and provide third-party companies with access to that view.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Scheduler to export the data on a regular basis to Cloud Storage, and provide third-party companies with access to the bucket.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a separate dataset in BigQuery that contains the relevant data to share, and provide third-party companies with access to the new dataset.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Cloud Dataflow job that reads the data in frequent time intervals, and writes it to the relevant BigQuery dataset or Cloud Storage bucket for third-party companies to use.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>You operate a database that stores stock trades and an application that retrieves average stock price for a given company over an adjustable window of time. The data is stored in Cloud Bigtable where the datetime of the stock trade is the beginning of the row key. Your application has thousands of concurrent users, and you notice that performance is starting to degrade as more stocks are added. What should you do to improve the performance of your application?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Change the row key syntax in your Cloud Bigtable table to begin with the stock symbol.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Change the row key syntax in your Cloud Bigtable table to begin with a random number per second.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Change the data pipeline to use BigQuery for storing stock trades, and update your application.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to write summary of each day’s stock trades to an Avro file on Cloud Storage.Update your application to read from Cloud Storage and Cloud Bigtable to compute the responses.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>

  <entry>
    <question>Each analytics team in your organization is running BigQuery jobs in their own projects. You want to enable each team to monitor slot usage within their projects. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a Stackdriver Monitoring dashboard based on the BigQuery metric query/scanned_bytes</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Stackdriver Monitoring dashboard based on the BigQuery metric slots/allocated_for_project</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a log export for each project, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Stackdriver Monitoring dashboard based on the custom metric</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create an aggregated log export at the organization level, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Stackdriver Monitoring dashboard based on the custom metric </detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question> 
You need to create a new transaction table in Cloud Spanner that stores product sales data. You are deciding what to use as a primary key. From a performance perspective, which strategy should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The current epoch time</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>A concatenation of the product name and the current epoch time</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>A random universally unique identifier number (version 4 UUID)</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The original order identification number from the sales system, which is a monotonically increasing integer</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>

  <entry>
    <question>Data Analysts in your company have the Cloud IAM Owner role assigned to them in their projects to allow them to work with multiple GCP products in their projects. Your organization requires that all BigQuery data access logs be retained for 6 months. You need to ensure that only audit personnel in your company can access the data access logs for all projects. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Enable data access logs in each Data Analyst’s projec</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Restrict access to Stackdriver Logging via Cloud IAM roles.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the data access logs via a project-level export sink to a Cloud Storage bucket in the Data Analysts’ project</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Restrict access to the Cloud Storage bucket.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the data access logs via a project-level export sink to a Cloud Storage bucket in a newly created projects for audit log F. Restrict access to the project with the exported logs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the data access logs via an aggregated export sink to a Cloud Storage bucket in a newly created project for audit log H. Restrict access to the project that contains the exported logs.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>Your company receives both batch- and stream-based event data. You want to process the data using Google Cloud Dataflow over a predictable time period. However, you realize that in some instances data can arrive late or out of order. How should you design your Cloud Dataflow pipeline to handle data that is late or out of order?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Set a single global window to capture all the data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Set sliding windows to capture all the lagged data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use watermarks and timestamps to capture the lagged data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Ensure every datasource type (stream or batch) has a timestamp, and use the timestamps to define the logic for lagged data.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>You need to create a near real-time inventory dashboard that reads the main inventory tables in your BigQuery data warehouse. Historical inventory data is stored as inventory balances by item and location. You have several thousand updates to inventory every hour. You want to maximize performance of the dashboard and ensure that the data is accurate. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Leverage BigQuery UPDATE statements to update the inventory balances as they are changing.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Partition the inventory balance table by item to reduce the amount of data scanned with each inventory update.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the BigQuery streaming the stream changes into a daily inventory movement tabl</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Calculate balances in a view that joins it to the historical inventory balance tabl</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Update the inventory balance table nightly.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the BigQuery bulk loader to batch load inventory changes into a daily inventory movement table.Calculate balances in a view that joins it to the historical inventory balance tabl</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Update the inventory balance table nightly.</detail>
        </option>
      </options>
    </answer>Answer:  A
  <entry>
    <question>You are implementing several batch jobs that must be executed on a schedule. These jobs have many interdependent steps that must be executed in a specific order. Portions of the jobs involve executing shell scripts, running Hadoop jobs, and running queries in BigQuery. The jobs are expected to run for many minutes up to several hours. If the steps fail, they must be retried a fixed number of times. Which service should you use to manage the execution of these jobs?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Cloud Scheduler</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Dataflow</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Functions</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Composer</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>You have a data stored in BigQuery. The data in the BigQuery dataset must be highly available. You need to define a storage, backup, and recovery strategy of this data that minimizes cost. How should you configure the BigQuery table?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Set the BigQuery dataset to be regiona</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>In the event of an emergency, use a point-in-time snapshot to recover the data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Set the BigQuery dataset to be regiona</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a scheduled query to make copies of the data to tables suffixed with the time of the backu E. In the event of an emergency, use the backup copy of the table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Set the BigQuery dataset to be multi-regiona</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>In the event of an emergency, use a point-in-time snapshot to recover the data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Set the BigQuery dataset to be multi-regiona</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a scheduled query to make copies of the data to tables suffixed with the time of the backu J. In the event of an emergency, use the backup copy of the table.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>You have an Apache Kafka Cluster on-prem with topics containing web application logs. You need to replicate the data to Google Cloud for analysis in BigQuery and Cloud Storage. The preferred replication method is mirroring to avoid deployment of Kafka Connect plugins.
What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Deploy a Kafka cluster on GCE VM Instance</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Configure your on-prem cluster to mirror your topics tothe cluster running in GC</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Deploy a Kafka cluster on GCE VM Instances with the PubSub Kafka connector configured as a Sink connecto E. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Deploy the PubSub Kafka connector to your on-prem Kafka cluster and configure PubSub as a Source connecto G. Use a Dataflow job to read fron PubSub and write to GCS.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Deploy the PubSub Kafka connector to your on-prem Kafka cluster and configure PubSub as a Sink connecto</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a Dataflow job to read fron PubSub and write to GCS.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>A data scientist has created a BigQuery ML model and asks you to create an ML pipeline to serve predictions. You have a REST API application with the requirement to serve predictions for an individual user ID with latency under 100 milliseconds. You use the following query to generate predictions: SELECT predicted_label, user_id FROM ML.PREDICT (MODEL ‘dataset.model’, table user_features). How should you create the ML pipeline?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Add a WHERE clause to the query, and grant the BigQuery Data Viewer role to the application service account.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create an Authorized View with the provided quer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Share the dataset that contains the view with the application service account.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Cloud Dataflow pipeline using BigQueryIO to read results from the quer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Grant the Dataflow Worker role to the application service account.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Cloud Dataflow pipeline using BigQueryIO to read predictions for all users from the query.Write the results to Cloud Bigtable using BigtableI G. Grant the Bigtable Reader role to the application service account so that the application can read predictions for individual users from Cloud Bigtable.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
 

</bank>