<?xml version="1.0" encoding="UTF-8"?>
<bank>
  <topic>PDE15 - Brain Dump (google.passguide.professional-data-engineer.pdf.exam.2023-aug-01.by.elton.196q.vce)</topic>
  <!-- STRUCTURE DEFINITION:
  </entry>
  <entry>
    <question>XXX</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>XXX</detail>
        </option>
      </options>
    </answer>
  </entry>
  -->

  <entry>
    <question>You are building new real-time data warehouse for your company and will use Google BigQuery streaming inserts. There is no guarantee that data will only be sent in once but you do have a unique ID for each row of data and an event timestamp. You want to ensure that duplicates are not included while interactively querying data. Which query type should you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Include ORDER BY DESK on timestamp column and LIMIT to 1.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use GROUP BY on the unique ID column and timestamp column and SUM on the values.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the LAG window function with PARTITION by unique ID along with WHERE LAG IS NOT NULL.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the ROW_NUMBER window function with PARTITION by unique ID along with WHERE row equals 1.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>

  <entry>
    <question>Your company is migrating their 30-node Apache Hadoop cluster to the cloud. They want to re-use Hadoop jobs they have already created and minimize the management of the cluster as much as possible. They also want to be able to persist data beyond the life of the cluster. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a Google Cloud Dataflow job to process the data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Google Cloud Dataproc cluster that uses persistent disks for HDFS.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Hadoop cluster on Google Compute Engine that uses persistent disks.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector. E. Create a Hadoop cluster on Google Compute Engine that uses Local SSD disks.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>

  <entry>
    <question>Business owners at your company have given you a database of bank transactions. Each row contains the user ID, transaction type, transaction location, and transaction amount. They ask you to investigate what type of machine learning can be applied to the data. Which three machine learning applications can you use? (Choose three.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Supervised learning to determine which transactions are most likely to be fraudulent.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Unsupervised learning to determine which transactions are most likely to be fraudulent.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Clustering to divide the transactions into N categories based on feature similarity.
</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Supervised learning to predict the location of a transaction.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reinforcement learning to predict the location of a transaction. F. Unsupervised learning to predict the location of a transaction.</detail>
        </option>
      </options>
    </answer>Answer:  BCE
  </entry>
  <entry>
    <question>You have Google Cloud Dataflow streaming pipeline running with a Google Cloud Pub/Sub subscription as the source. You need to make an update to the code that will make the new Cloud Dataflow pipeline incompatible with the current version. You do not want to lose any data when making this update. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Update the current pipeline and use the drain flag.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Update the current pipeline and provide the transform mapping JSON object.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a new pipeline that has the same Cloud Pub/Sub subscription and cancel the old pipeline.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a new pipeline that has a new Cloud Pub/Sub subscription and cancel the old pipeline.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>

  <entry>
    <question>Your company is using WHILECARD tables to query data across multiple tables with similar names. The SQL statement is currently failing with the following error: # Syntax error : Expected end of statement but got “-“ at [4:11] SELECT age
FROM
bigquery-public-data.noaa_gsod.gsod WHERE
age != 99
AND_TABLE_SUFFIX = ‘1929’ ORDER BY
age DESC
Which table name will make the SQL statement work correctly?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>‘bigquery-public-data.noaa_gsod.gsod‘</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>bigquery-public-data.noaa_gsod.gsod*</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>‘bigquery-public-data.noaa_gsod.gsod’*</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>‘bigquery-public-data.noaa_gsod.gsod*`</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>
You create an important report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. You notice that visualizations are not showing data that is less than 1 hour old. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Disable caching by editing the report settings.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Disable caching in BigQuery by editing table details.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Refresh your browser tab showing the visualizations.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Clear your browser history for the past hour then reload the tab showing the virtualizations.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>Your weather app queries a database every 15 minutes to get the current temperature. The frontend is powered by Google App Engine and server millions of users. How should you design the frontend to respond to a database failure?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Issue a command to restart the database servers.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Retry the query with exponential backoff, up to a cap of 15 minutes.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Retry the query every second until it comes back online to minimize staleness of data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reduce the query frequency to once every hour until the database comes back online.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>Flowlogistic is rolling out their real-time inventory tracking system. The tracking devices will all send package-tracking messages, which will now go to a single Google Cloud Pub/Sub topic instead of the Apache Kafka cluster. A subscriber application will then process the messages for real-time reporting and store them in Google BigQuery for historical analysis. You want to ensure the package data can be analyzed over time.
Which approach should you take?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Attach the timestamp on each message in the Cloud Pub/Sub subscriber application as they are received.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Attach the timestamp and Package ID on the outbound message from each publisher device as they are sent to Clod Pub/Sub.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the NOW () function in BigQuery to record the event’s time.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the automatically generated timestamp from Cloud Pub/Sub to order the data.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>MJTelco needs you to create a schema in Google Bigtable that will allow for the historical analysis of the last 2 years of records. Each record that comes in is sent every 15 minutes, and contains a unique identifier of the device and a data record. The most common query is for all the data for a given device for a given day. Which schema should you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Rowkey: date#device_idColumn data: data_point</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Rowkey: dateColumn data: device_id, data_point</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Rowkey: device_idColumn data: date, data_point</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Rowkey: data_pointColumn data: device_id, date E. Rowkey: date#data_pointColumn data: device_id</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>

  <entry>
    <question> 
MJTelco’s Google Cloud Dataflow pipeline is now ready to start receiving data from the 50,000 installations. You want to allow Cloud Dataflow to scale its compute power up as required. Which Cloud Dataflow pipeline configuration setting should you update?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The zone</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The number of workers</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The disk size per worker</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The maximum number of workers</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>

  <entry>
    <question>You work for a large fast food restaurant chain with over 400,000 employees. You store employee information in Google BigQuery in a Users table consisting of a FirstName field and a LastName field. A member of IT is building an application and asks you to modify the schema and data in BigQuery so the application can query a FullName field consisting of the value of the FirstName field concatenated with a space, followed by the value of the LastName field for each employee. How can you make that data available while minimizing cost?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a view in BigQuery that concatenates the FirstName and LastName field values to produce the FullName.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a new column called FullName to the Users tabl</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Run an UPDATE statement that updates the FullName column for each user with the concatenation of the FirstName and LastName values.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Google Cloud Dataflow job that queries BigQuery for the entire Users table, concatenates the FirstName value and LastName value for each user, and loads the proper values for FirstName, LastName, and FullName into a new table in BigQuery.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use BigQuery to export the data for the table to a CSV fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Google Cloud Dataproc job to process the CSV file and output a new CSV file containing the proper values for FirstName, LastName and FullNam</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Run a BigQuery load job to load the new CSV file into BigQuery.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>You work for an economic consulting firm that helps companies identify economic trends as they happen. As part of your analysis, you use Google BigQuery to correlate customer data with the average prices of the 100 most common goods sold, including bread, gasoline, milk, and others. The average prices of these goods are updated every 30 minutes. You want to make sure this data stays up to date so you can combine it with other data in BigQuery as cheaply as possible. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Load the data every 30 minutes into a new partitioned table in BigQuery.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store and update the data in a regional Google Cloud Storage bucket and create a federated data source in BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store the data in Google Cloud Datastor</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Google Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Cloud Datastore E. Store the data in a file in a regional Google Cloud Storage bucke</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Google Cloud Storage.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>When you design a Google Cloud Bigtable schema it is recommended that you .</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Avoid schema designs that are based on NoSQL concepts</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create schema designs that are based on a relational database design</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Avoid schema designs that require atomicity across rows</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create schema designs that require atomicity across rows</detail>
        </option>
      </options>
    </answer>Answer:  C

Which of the following job types are supported by Cloud Dataproc (select 3 answers)?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Hive</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Pig</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>YARN</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Spark</detail>
        </option>
      </options>
    </answer>Answer:  ABD
  </entry>
  <entry>
    <question>Why do you need to split a machine learning dataset into training data and test data?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>So you can try two different sets of features</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>To make sure your model is generalized for more than just the training data</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>To allow you to create unit tests in your code</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>So you can use one dataset for a wide model and one for a deep model</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>What is the recommended action to do in order to switch between SSD and HDD storage for your Google Cloud Bigtable instance?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>create a third instance and sync the data from the two storage types via batch jobs</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>export the data from the existing instance and import the data into a new instance</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>run parallel instances where one is HDD and the other is SDD</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>the selection is final and you must resume using the same storage type</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question></detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>you're running a performance test that depends upon Cloud Bigtable, all the choices except one below are recommended steps. Which is NOT a recommended step to follow?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Do not use a production instance.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Run your test for at least 10 minutes.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Before you test, run a heavy pre-test for several minutes.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use at least 300 GB of data.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>What Dataflow concept determines when a Window's contents should be output based on certain criteria being met?
   </entry>
  <entry>
    <question> A. Sessions</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>OutputCriteria</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Windows</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Triggers</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>Does Dataflow process batch data pipelines or streaming data pipelines?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Only Batch Data Pipelines</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Both Batch and Streaming Data Pipelines</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Only Streaming Data Pipelines</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>None of the above</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>Which Google Cloud Platform service is an alternative to Hadoop with Hive?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Cloud Dataflow</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Bigtable</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Datastore</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>To run a TensorFlow training job on your own computer using Cloud Machine Learning Engine, what would your command start with?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>gcloud ml-engine local train</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>gcloud ml-engine jobs submit training</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>gcloud ml-engine jobs submit training local</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You can't run a TensorFlow program on your own computer using Cloud ML Engine .</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>

  <entry>
    <question>Which of the following statements is NOT true regarding Bigtable access roles?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Using IAM roles, you cannot give a user access to only one table in a project, rather than all tables in a project.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>To give a user access to only one table in a project, grant the user the Bigtable Editor role for that table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You can configure access control only at the project level.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>To give a user access to only one table in a project, you must configure access through your application.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>Google Cloud Bigtable indexes a single value in each row. This value is called the .</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>primary key</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>unique key</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>row key</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>master key</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>Which row keys are likely to cause a disproportionate number of reads and/or writes on a particular node in a Bigtable cluster (select 2 answers)?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>A sequential numeric ID</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>A timestamp followed by a stock symbol</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>A non-sequential numeric ID</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>A stock symbol followed by a timestamp</detail>
        </option>
      </options>
    </answer>Answer:  AB
  </entry>
  <entry>
    <question>Which of these statements about BigQuery caching is true?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>By default, a query's results are not cached.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>BigQuery caches query results for 48 hours.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Query results are cached even if you specify a destination table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>There is no charge for a query that retrieves its results from cache.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>How would you query specific partitions in a BigQuery table?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use the DAY column in the WHERE clause</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the EXTRACT(DAY) clause</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the PARTITIONTIME pseudo-column in the WHERE clause</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use DATE BETWEEN in the WHERE clause</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>

  <entry>
    <question>Which of the following are feature engineering techniques? (Select 2 answers)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Hidden feature layers</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Feature prioritization</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Crossed feature columns</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Bucketization of a continuous feature</detail>
        </option>
      </options>
    </answer>Answer:  CD
  </entry>
  <entry>
    <question>Which of the following are examples of hyperparameters? (Select 2 answers.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Number of hidden layers</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Number of nodes in each hidden layer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Biases</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Weights</detail>
        </option>
      </options>
    </answer>Answer:  AB
  </entry>
  <entry>
    <question>2

Which of the following is NOT a valid use case to select HDD (hard disk drives) as the storage for Google Cloud Bigtable?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>You expect to store at least 10 TB of data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You will mostly run batch workloads with scans and writes, rather than frequently executing random reads of a small number of rows.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You need to integrate with Google BigQuery.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You will not use the data to back a user-facing or latency-sensitive application.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>6

For the best possible performance, what is the recommended zone for your Compute Engine instance and Cloud Bigtable instance?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Have the Compute Engine instance in the furthest zone from the Cloud Bigtable instance.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Have both the Compute Engine instance and the Cloud Bigtable instance to be in different zones.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Have both the Compute Engine instance and the Cloud Bigtable instance to be in the same zone.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Have the Cloud Bigtable instance to be in the same zone as all of the consumers of your data.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>

  <entry>
    <question>7

You are developing a software application using Google's Dataflow SDK, and want to use conditional, for loops and other complex programming structures to create a branching pipeline. Which component will be used for the data processing operation?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>PCollection</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Transform</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Pipeline</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Sink API</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>9

Which is the preferred method to use to avoid hotspotting in time series data in Bigtable?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Field promotion</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Randomization</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Salting</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Hashing</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>

  <entry>
    <question>7

You are planning to use Google's Dataflow SDK to analyze customer data such as displayed below. Your project requirement is to extract only the customer name from the data source and then write to an output PCollection.
Tom,555 X street Tim,553 Y street Sam, 111 Z street
Which operation is best suited for the above data processing requirement?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>ParDo</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Sink API</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Source API</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Data extraction</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>8

Which of these numbers are adjusted by a neural network as it learns from a training dataset (select 2 answers)?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Weights</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Biases</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Continuous features</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Input values</detail>
        </option>
      </options>
    </answer>Answer:  AB
  </entry>

  <entry>
    <question>6

You have a data pipeline with a Cloud Dataflow job that aggregates and writes time series metrics to Cloud Bigtable. This data feeds a dashboard used by thousands of users across the organization. You need to support additional concurrent users and reduce the amount of time required to write the data. Which two actions should you take? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Configure your Cloud Dataflow pipeline to use local execution</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the maximum number of Cloud Dataflow workers by setting maxNumWorkers in PipelineOptions</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the number of nodes in the Cloud Bigtable cluster</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Modify your Cloud Dataflow pipeline to use the Flatten transform before writing to Cloud Bigtable</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Modify your Cloud Dataflow pipeline to use the CoGroupByKey transform before writing to Cloud Bigtable</detail>
        </option>
      </options>
    </answer>Answer:  DE
  </entry>
  <entry>
    <question>8

You work on a regression problem in a natural language processing domain, and you have 100M labeled exmaples in your dataset. You have randomly shuffled your data and split your dataset into train and test samples (in a 90/10 ratio). After you trained the neural network and evaluated your model on a test set, you discover that the root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set. How should you improve the performance of your model?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Increase the share of the test sample in the train-test split.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Try to collect more data and increase the size of your dataset.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Try out regularization techniques (e.g., dropout of batch normalization) to avoid overfitting.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the complexity of your model by, e.g., introducing an additional layer or increase sizing the size of vocabularies or n-grams used.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>1

Your infrastructure includes a set of YouTube channels. You have been tasked with creating a process for sending the YouTube channel data to Google Cloud for analysis. You want to design a solution that allows your world-wide marketing teams to perform ANSI SQL and other types of analysis on up-to-date YouTube channels log data. How should you set up the log data transfer into Google Cloud?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Regional bucket as a final destination.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Regional storage bucket as a final destination.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>

  <entry>
    <question>1
</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>your organization expands its usage of GCP, many teams have started to create their own projects. Projects are further multiplied to accommodate different stages of deployments and target audiences. Each project requires unique access control configurations. The central IT team needs to have access to all projects. Furthermore, data from Cloud Storage buckets and BigQuery datasets must be shared for use in other projects in an ad hoc way. You want to simplify access control management by minimizing the number of policies. Which two steps should you take? Choose 2 answers.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Deployment Manager to automate access provision.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Introduce resource hierarchy to leverage access control policy inheritance.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create distinct groups for various teams, and specify groups in Cloud IAM policies.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Only use service accounts when sharing data for Cloud Storage buckets and BigQuery datasets.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>For each Cloud Storage bucket or BigQuery dataset, decide which projects need acces</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Find all the active members who have access to these projects, and create a Cloud IAM policy to grant access to all these users.</detail>
        </option>
      </options>
    </answer>Answer:  AC
  </entry>
  <entry>
    <question>0

Your analytics team wants to build a simple statistical model to determine which customers are most likely to work with your company again, based on a few different metrics. They want to run the model on Apache Spark, using data housed in Google Cloud Storage, and you have recommended using Google Cloud Dataproc to execute this job. Testing has shown that this workload can run in approximately 30 minutes on a 15-node cluster, outputting the results into Google BigQuery. The plan is to run this workload weekly. How should you optimize the cluster for cost?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Migrate the workload to Google Cloud Dataflow</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use pre-emptible virtual machines (VMs) for the cluster</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a higher-memory node so that the job runs faster</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use SSDs on the worker nodes so that the job can run faster</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>2

You are a retailer that wants to integrate your online sales capabilities with different in-home assistants, such as Google Home. You need to interpret customer voice commands and issue an order to the backend systems. Which solutions should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Cloud Speech-to-Text API</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Natural Language API</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dialogflow Enterprise Edition</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud AutoML Natural Language</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>

  <entry>
    <question>0

You need to move 2 PB of historical data from an on-premises storage appliance to Cloud Storage within six months, and your outbound network capacity is constrained to 20 Mb/sec. How should you migrate this data to Cloud Storage?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Transfer Appliance to copy the data to Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use gsutil cp –J to compress the content being uploaded to Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a private URL for the historical data, and then use Storage Transfer Service to copy the data to Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use trickle or ionice along with gsutil cp to limit the amount of bandwidth gsutil utilizes to less than 20 Mb/sec so it does not interfere with the production traffic</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>1

You want to automate execution of a multi-step data pipeline running on Google Cloud. The pipeline includes Cloud Dataproc and Cloud Dataflow jobs that have multiple dependencies on each other. You want to use managed services where possible, and the pipeline will run every day. Which tool should you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>cron</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Composer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Scheduler</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Workflow Templates on Cloud Dataproc</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>6

You are integrating one of your internal IT applications and Google BigQuery, so users can query BigQuery from the application’s interface. You do not want individual users to authenticate to BigQuery and you do not want to give them access to the dataset. You need to securely access BigQuery from your IT application.
What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create groups for your users and give those groups access to the dataset</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Integrate with a single sign-on (SSO) platform, and pass each user’s credentials along with the query request</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a service account and grant dataset access to that accoun</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the service account’s private key to access the dataset</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a dummy user and grant dataset access to that use</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store the username and password for that user in a file on the files system, and use those credentials to access the BigQuery dataset</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>9

You are designing storage for 20 TB of text files as part of deploying a data pipeline on Google Cloud. Your input data is in CSV format. You want to minimize the cost of querying aggregate values for multiple users who will query the data in Cloud Storage with multiple engines. Which storage service and schema design should you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Bigtable for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install the HBase shell on a Compute Engine instance to query the Cloud Bigtable data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Bigtable for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Link as permanent tables in BigQuery for query.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Storage for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Link as permanent tables in BigQuery for query.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Storage for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Link as temporary tables in BigQuery for query.
 </detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>4

You are managing a Cloud Dataproc cluster. You need to make should you do?
a job run faster while minimizing costs, without losing work in progress on your clusters. What
configure them to forcefully decommission.
use Cloud Stackdriver to trigger a script to preserve work. configure them to use graceful decommissioning.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Increase the</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>6

You are designing an Apache Beam pipeline to enrich data from Cloud Pub/Sub with static reference data from BigQuery. The reference data is small enough to fit in memory on a single worker. The pipeline should write enriched results to BigQuery for analysis. Which job type and transforms should this pipeline use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Batch job, PubSubIO, side-inputs</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Streaming job, PubSubIO, JdbcIO, side-outputs</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Streaming job, PubSubIO, BigQueryIO, side-inputs</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Streaming job, PubSubIO, BigQueryIO, side-outputs</detail>
        </option>
      </options>
    </answer>Answer:  A

  </entry>
  <entry>
    <question>3

You want to build a managed Hadoop system as your data lake. The data transformation process is composed of a series of Hadoop jobs executed in sequence. To accomplish the design of separating storage from compute, you decided to use the Cloud Storage connector to store all input data, output data, and intermediary data. However, you noticed that one Hadoop job runs very slowly with Cloud Dataproc, when compared with the on-premises bare-metal Hadoop environment (8-core nodes with 100-GB RAM). Analysis shows that this particular Hadoop job is disk I/O intensive. You want to resolve the issue. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Allocate sufficient memory to the Hadoop cluster, so that the intermediary data of that particular Hadoop job can be held in memory</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Allocate sufficient persistent disk space to the Hadoop cluster, and store the intermediate data of that particular Hadoop job on native HDFS</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Allocate more CPU cores of the virtual machine instances of the Hadoop cluster so that the networking bandwidth for each instance can scale up</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Allocate additional network interface card (NIC), and configure link aggregation in the operating system to use the combined throughput when working with Cloud Storage</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>8

You work for a manufacturing company that sources up to 750 different components, each from a different supplier. You’ve collected a labeled dataset that has on average 1000 examples for each unique component. Your team wants to implement an app to help warehouse workers recognize incoming components based on a photo of the component. You want to implement the first working version of this app (as Proof-Of-Concept) within a few working days. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Vision AutoML with the existing dataset.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Vision AutoML, but reduce your dataset twice.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Vision API by providing custom labels as recognition hints.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Train your own image recognition model leveraging transfer learning techniques.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>9

You need to set access to BigQuery for different departments within your company. Your solution should comply with the following requirements:
Each department should have access only to their data.
Each department will have one or more leads who need to be able to create and update tables and provide them to their team.
Each department has data analysts who need to be able to query but not modify data. How should you set access to the data in BigQuery?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a dataset for each departmen</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Assign the department leads the role of OWNER, and assign the data analysts the role of WRITER on their dataset.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a dataset for each departmen</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Assign the department leads the role of WRITER, and assign the data analysts the role of READER on their dataset. E. Create a table for each departmen
   </entry>
  <entry>
    <question>8
</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Assign the department leads the role of Owner, and assign the data analysts the role of Editor on the project the table is in. G. Create a table for each departmen</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Assign the department leads the role of Editor, and assign the data analysts the role of Viewer on the project the table is in.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>4

You used Cloud Dataprep to create a recipe on a sample of data in a BigQuery table. You want to reuse this recipe on a daily upload of data with the same schema, after the load job with variable execution time completes. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a cron schedule in Cloud Dataprep.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create an App Engine cron job to schedule the execution of the Cloud Dataprep job.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the recipe as a Cloud Dataprep template, and create a job in Cloud Scheduler.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the Cloud Dataprep job as a Cloud Dataflow template, and incorporate it into a Cloud Composer job.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
 
  <entry>
    <question>2

You launched a new gaming app almost three years ago. You have been uploading log files from the previous day to a separate Google BigQuery table with the table name format LOGS_yyyymmdd. You have been using table wildcard functions to generate daily and monthly reports for all time ranges. Recently, you discovered that some queries that cover long date ranges are exceeding the limit of 1,000 tables and failing. How can you resolve this issue?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Convert all daily log tables into date-partitioned tables</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Convert the sharded tables into a single partitioned table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Enable query caching so you can cache data from previous months</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create separate views to cover each month, and query from these views</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>7

Your company is selecting a system to centralize data ingestion and delivery. You are considering messaging and data integration systems to address the requirements. The key requirements are:
The ability to seek to a particular offset in a topic, possibly back to the start of all data ever captured
Support for publish/subscribe semantics on hundreds of topics
Retain per-key ordering Which system should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Apache Kafka</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Pub/Sub</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Firebase Cloud Messaging </detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>2

Your company is currently setting up data pipelines for their campaign. For all the Google Cloud Pub/Sub streaming data, one of the important business requirements is to be able to periodically identify the inputs and their timings during their campaign. Engineers have decided to use windowing and transformation in Google Cloud Dataflow for this purpose. However, when testing this feature, they find that the Cloud Dataflow job fails for the all streaming insert. What is the most likely cause of this problem?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>They have not assigned the timestamp, which causes the job to fail</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>They have not set the triggers to accommodate the data coming in late, which causes the job to fail</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>They have not applied a global windowing function, which causes the job to fail when the pipeline is created</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>They have not applied a non-global windowing function, which causes the job to fail when the pipeline is created</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>5

You are deploying MariaDB SQL databases on GCE VM Instances and need to configure monitoring and alerting. You want to collect metrics including network connections, disk IO and replication status from MariaDB with minimal development effort and use StackDriver for dashboards and alerts.
What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Install the OpenCensus Agent and create a custom metric collection application with a StackDriver exporter.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Place the MariaDB instances in an Instance Group with a Health Check.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install the StackDriver Logging Agent and configure fluentd in_tail plugin to read MariaDB logs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install the StackDriver Agent and configure the MySQL plugin.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>0

You have Cloud Functions written in Node.js that pull messages from Cloud Pub/Sub and send the data to BigQuery. You observe that the message processing rate on the Pub/Sub topic is orders of magnitude higher than anticipated, but there is no error logged in Stackdriver Log Viewer. What are the two most likely causes of this problem? Choose 2 answers.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Publisher throughput quota is too small.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Total outstanding messages exceed the 10-MB maximum.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Error handling in the subscriber code is not handling run-time errors properly.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The subscriber code cannot keep up with the messages.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The subscriber code does not acknowledge the messages that it pulls.</detail>
        </option>
      </options>
    </answer>Answer:  CD
  </entry>
  <entry>
    <question>4

The marketing team at your organization provides regular updates of a segment of your customer dataset. The marketing team has given you a CSV with 1 million records that must be updated in BigQuery. When you use the UPDATE statement in BigQuery, you receive a quotaExceeded error. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Reduce the number of records updated each day to stay within the BigQuery UPDATE DML statement limit.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the BigQuery UPDATE DML statement limit in the Quota management section of the Google Cloud Platform Console.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Split the source CSV file into smaller CSV files in Cloud Storage to reduce the number of BigQuery UPDATE DML statements per BigQuery job.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Import the new records from the CSV file into a new BigQuery tabl</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a BigQuery job that merges the new records with the existing records and writes the results to a new BigQuery table.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>8

You work for an advertising company, and you’ve developed a Spark ML model to predict click-through rates at advertisement blocks. You’ve been developing everything at your on-premises data center, and now your company is migrating to Google Cloud. Your data center will be migrated to BigQuery. You periodically retrain your Spark ML models, so you need to migrate existing training pipelines to Google Cloud. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud ML Engine for training existing Spark ML models</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Rewrite your models on TensorFlow, and start using Cloud ML Engine</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataproc for training existing Spark ML models, but start reading data directly from BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Spin up a Spark cluster on Compute Engine, and train Spark ML models on the data exported from BigQuery</detail>
        </option>
      </options>
    </answer>Answer:  A


You are creating a new pipeline in Google Cloud to stream IoT data from Cloud Pub/Sub through Cloud Dataflow to BigQuery. While previewing the data, you notice that roughly 2% of the data appears to be corrupt. You need to modify the Cloud Dataflow pipeline to filter out this corrupt data. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Add a SideInput that returns a Boolean if the element is corrupt.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a ParDo transform in Cloud Dataflow to discard corrupt elements.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a Partition transform in Cloud Dataflow to separate valid data from corrupt data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a GroupByKey transform in Cloud Dataflow to group all of the valid data together and discard the rest.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>1

You work for a shipping company that has distribution centers where packages move on delivery lines to route them properly. The company wants to add cameras to the delivery lines to detect and track any visual damage to the packages in transit. You need to create a way to automate the detection of damaged packages and flag them for human review in real time while the packages are in transit. Which solution should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use BigQuery machine learning to be able to train the model at scale, so you can analyze the packages in batches.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Train an AutoML model on your corpus of images, and build an API around that model to integrate with the package tracking applications.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the Cloud Vision API to detect for damage, and raise an alert through Cloud Function</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Integrate the package tracking applications with this function.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use TensorFlow to create a model that is trained on your corpus of image</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Python notebook in Cloud Datalab that uses this model so you can analyze for damaged packages.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>3

You are migrating your data warehouse to BigQuery. You have migrated all of your data into tables in a dataset. Multiple users from your organization will be using the data. They should only see certain tables based on their team membership. How should you set user permissions?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Assign the users/groups data viewer access at the table level for each table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create SQL views for each team in the same dataset in which the data resides, and assign the users/groups data viewer access to the SQL views</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create authorized views for each team in the same dataset in which the data resides, and assign theusers/groups data viewer access to the authorized views</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create authorized views for each team in datasets created for each tea</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Assign the authorized views data viewer access to the dataset in which the data reside</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Assign the users/groups data viewer access to the datasets in which the authorized views reside</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>8

You are building a new data pipeline to share data between two different types of applications: jobs generators and job runners. Your solution must scale to accommodate increases in usage and must accommodate the addition of new applications without negatively affecting the performance of existing ones. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create an API using App Engine to receive and send messages to the applications</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a Cloud Pub/Sub topic to publish jobs, and use subscriptions to execute them</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a table on Cloud SQL, and insert and delete rows with the job information</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a table on Cloud Spanner, and insert and delete rows with the job information</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>


</bank>