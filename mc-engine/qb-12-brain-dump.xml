<?xml version="1.0" encoding="UTF-8"?>
<bank>
  <topic>PDE12 - Brain Dump (google.exambible.professional-data-engineer.pdf.exam.2023-oct-16.by.baron.100q.vce)</topic>
  <!-- STRUCTURE DEFINITION:
  </entry>
  <entry>
    <question>XXX</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>XXX</detail>
        </option>
      </options>
    </answer>
  </entry>
  -->

  <entry>
    <question>Your company produces 20,000 files every hour. Each data file is formatted as a comma separated values (CSV) file that is less than 4 KB. All files must be ingested on Google Cloud Platform before they can be processed. Your company site has a 200 ms latency to Google Cloud, and your Internet connection bandwidth is limited as 50 Mbps. You currently deploy a secure FTP (SFTP) server on a virtual machine in Google Compute Engine as the data ingestion point. A local SFTP client runs on a dedicated machine to transmit the CSV files as is. The goal is to make reports with data from the previous day available to the executives by 10:00 a.m. each day. This design is barely able to keep up with the current volume, even though the bandwidth utilization is rather low.
You are told that due to seasonality, your company expects the number of files to double for the next three months. Which two actions should you take? (choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Introduce data compression for each file to increase the rate file of file transfer.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Contact your internet service provider (ISP) to increase your maximum bandwidth to at least 100 Mbps.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Redesign the data ingestion process to use gsutil tool to send the CSV files to a storage bucket in parallel.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Assemble 1,000 files into a tape archive (TAR) fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Transmit the TAR files instead, and disassemble the CSV files in the cloud upon receiving them.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create an S3-compatible storage endpoint in your network, and use Google Cloud Storage Transfer Service to transfer on-premices data to the designated storage bucket.</detail>
        </option>
      </options>
    </answer>Answer:  CE


Your company is loading comma-separated values (CSV) files into Google BigQuery. The data is fully imported successfully; however, the imported data is not matching byte-to-byte to the source file. What is the most likely cause of this problem?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The CSV data loaded in BigQuery is not flagged as CSV.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The CSV data has invalid rows that were skipped on import.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The CSV data loaded in BigQuery is not using BigQueryâ€™s default encoding.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The CSV data has not gone through an ETL phase before loading into BigQuery.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>

  <entry>
    <question>Which of the following is NOT a valid use case to select HDD (hard disk drives) as the storage for Google Cloud Bigtable?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>You expect to store at least 10 TB of data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You will mostly run batch workloads with scans and writes, rather than frequently executing random reads of a small number of rows.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You need to integrate with Google BigQuery.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You will not use the data to back a user-facing or latency-sensitive application.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>

</bank>