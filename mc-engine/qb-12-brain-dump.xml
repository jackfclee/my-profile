<?xml version="1.0" encoding="UTF-8"?>
<bank>
  <topic>PDE17 - Brain Dump (google.passleader.professional-data-engineer.brain.dumps.2021-jul-04.by.fitch.56q.vce)</topic>
  <!-- STRUCTURE DEFINITION:
  </entry>
  <entry>
    <question>XXX</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>XXX</detail>
        </option>
      </options>
    </answer>
  </entry>
  -->

  <entry>
    <question>You want to use a database of information about tissue samples to classify future tissue samples as either normal or mutated. You are evaluating an unsupervised anomaly detection method for classifying the tissue samples. Which two characteristic support this method? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>There are very few occurrences of mutations relative to normal samples.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>There are roughly equal occurrences of both normal and mutated samples in the database.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>You expect future mutations to have different features from the mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You expect future mutations to have similar features to the mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You already have labels for which samples are mutated and which are normal in the database.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your company is running their first dynamic campaign, serving different offers by analyzing real-time data during the holiday season. The data scientists are collecting terabytes of data that rapidly grows every hour during their 30-day campaign. They are using Google Cloud Dataflow to preprocess the data and collect the feature (signals) data that is needed for the machine learning model in Google Cloud Bigtable. The team is observing suboptimal performance with reads and writes of their initial load of 10 TB of data. They want to improve this performance while minimizing cost. What should they do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Redefine the schema by evenly distributing reads and writes across the row space of the table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The performance issue should be resolved over time as the site of the BigDate cluster is increased.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Redesign the schema to use a single row key to identify values that need to be updated frequently in the cluster.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Redesign the schema to use row keys based on numeric IDs that increase sequentially per user viewing the offers.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Flowlogistic’s management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system. You need to build a new system on Google Cloud Platform (GCP) that will feed the proprietary tracking software. The system must be able to ingest data from a variety of global sources, process and query in real-time, and store the data reliably. Which combination of GCP products should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Pub/Sub, Cloud Dataflow, and Local SSD</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Cloud Pub/Sub, Cloud SQL, and Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Load Balancing, Cloud Dataflow, and Cloud Storage</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You create a new report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. It is company policy to ensure employees can view only the data associated with their region, so you create and populate a table for each region. You need to enforce the regional access policy to the data. Which two actions should you take? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Ensure all the tables are included in global dataset.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Ensure each table is included in a dataset for a region.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Adjust the settings for each table to allow a related region-based security group view access.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Adjust the settings for each view to allow a related region-based security group view access.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Adjust the settings for each dataset to allow a related region-based security group view access.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>All Google Cloud Bigtable client requests go through a front-end server they are sent to a Cloud Bigtable node.</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>before</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>after</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>only if</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>once</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which of these is NOT a way to customize the software on Dataproc cluster instances?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Set initialization actions</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Modify configuration files using cluster properties</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Configure the cluster using Cloud Deployment Manager</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Log into the master node and make changes from there</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which of these operations can you perform from the BigQuery Web UI?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Upload a file in SQL format.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Load data with nested and repeated fields.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload a 20 MB file.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload multiple files using a wildcard.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>What are two methods that can be used to denormalize tables in BigQuery?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>1) Split table into multiple tables; 2) Use a partitioned table</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>1) Join tables into one table; 2) Use nested repeated fields</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>1) Use a partitioned table; 2) Join tables into one table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>1) Use nested repeated fields; 2) Use a partitioned table</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which role must be assigned to a service account used by the virtual machines in a Dataproc cluster so they can execute jobs?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Dataproc Worker</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataproc Viewer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataproc Runner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataproc Editor</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which Java SDK class can you use to run your Dataflow programs locally?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>LocalRunner</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>DirectPipelineRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>MachineRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>LocalPipelineRunner</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which is not a valid reason for poor Cloud Bigtable performance?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The workload isn't appropriate for Cloud Bigtable.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The table's schema is not designed correctly.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>The Cloud Bigtable cluster has too many nodes.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>There are issues with the network connection.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Scaling a Cloud Dataproc cluster typically involves .</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>increasing or decreasing the number of worker nodes</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>increasing or decreasing the number of master nodes</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>moving memory to run more applications on a single node</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>deleting applications from unused nodes periodically</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>To give a user read permission for only the first three columns of a table, which access control method would you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Primitive role</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Predefined role</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Authorized view</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>It's not possible to give access to only the first three columns of a table.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You want to use a BigQuery table as a data sink. In which writing mode(s) can you use BigQuery as a sink?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Both batch and streaming</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>BigQuery cannot be used as a sink</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Only batch</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Only streaming</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cloud Dataproc is a managed Apache Hadoop and Apache service.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Blaze</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Spark</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Fire</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Ignite</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which action can a Cloud Dataproc Viewer perform?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Submit a job.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a cluster.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Delete a cluster.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>List the jobs.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are running a pipeline in Cloud Dataflow that receives messages from a Cloud Pub/Sub topic and writes the results to a BigQuery dataset in the EU. Currently, your pipeline is located in europe-west4 and has a maximum of 3 workers, instance type n1-standard-1. You notice that during peak periods, your pipeline is struggling to process records in a timely fashion, when all 3 workers are at maximum CPU utilization. Which two actions can you take to increase performance of your pipeline? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Increase the number of max workers</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use a larger instance type for your Cloud Dataflow workers</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Change the zone of your Cloud Dataflow pipeline to run in us-central1</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a temporary table in Cloud Bigtable that will act as a buffer for new dat</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Cloud Bigtable to BigQuery F. Create a temporary table in Cloud Spanner that will act as a buffer for new dat</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Cloud Spanner to BigQuery</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to implement a change that would improve query performance in BigQuery. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Implement clustering in BigQuery on the ingest date column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Implement clustering in BigQuery on the package-tracking ID column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Tier older data onto Cloud Storage files, and leverage extended tables.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Re-create the table using data partitioning on the package delivery date.</detail>
        </option>
      </options>
    </answer>
   </entry>
  <entry>
    <question>You have a requirement to insert minute-resolution data from 50,000 sensors into a BigQuery table. You expect significant growth in data volume and need the data to be available within 1 minute of ingestion for real-time analysis of aggregated trends. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use bq load to load a batch of sensor data every 60 seconds.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a Cloud Dataflow pipeline to stream data into the BigQuery table.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use the INSERT statement to insert a batch of data every 60 seconds.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the MERGE statement to apply updates in batch every 60 seconds.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You’re training a model to predict housing prices based on an available dataset with real estate properties. Your plan is to train a fully connected neural net, and you’ve discovered that the dataset contains latitude and longtitude of the property. Real estate professionals have told you that the location of the property is highly influential on price, so you’d like to engineer a feature that incorporates this physical dependency. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Provide latitude and longtitude as input vectors to your neural net.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Create a numeric column from a feature cross of latitude and longtitude.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a feature cross of latitude and longtitude, bucketize at the minute level and use L1 regularization during optimization.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a feature cross of latitude and longtitude, bucketize it at the minute level and use L2 regularization during optimization.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are designing storage for two relational tables that are part of a 10-TB database on Google Cloud. You want to support transactions that scale horizontally. You also want to optimize data for range queries on nonkey columns. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud SQL for storage. Add secondary indexes to support query patterns.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to transform data to support query patterns.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Spanner for storage. Add secondary indexes to support query patterns.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use Cloud Dataflow to transform data to support query patterns.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have a data pipeline that writes data to Cloud Bigtable using well-designed row keys. You want to monitor your pipeline to determine when to increase the size of you Cloud Bigtable cluster. Which two actions can you take to accomplish this? Choose 2 answers.</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Review Key Visualizer metric. Increase the size of the Cloud Bigtable cluster when the Read pressure index is above 100.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Review Key Visualizer metric. Increase the size of the Cloud Bigtable cluster when the Write pressure index is above 100.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Monitor the latency of write operation. Increase the size of the Cloud Bigtable cluster when there is a sustained increase in write latency.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Monitor storage utilization. Increase the size of the Cloud Bigtable cluster when utilization increases above 70% of max capacity.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Monitor latency of read operation. Increase the size of the Cloud Bigtable cluster of read operations take longer than 100 ms.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Government regulations in your industry mandate that you have to maintain an auditable record of access to certain types of datA. Assuming that all expiring logs will be archived correctly, where should you store data that is subject to that mandate?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Encrypted on Cloud Storage with user-supplied encryption key</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>A separate decryption key will be given to each authorized user.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>In Cloud SQL, with separate database user names to each use</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The Cloud SQL Admin activity logs will be used to provide the auditability.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>In a bucket on Cloud Storage that is accessible only by an AppEngine service that collects user information and logs the access before providing a link to the bucket.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have a query that filters a BigQuery table using a WHERE clause on timestamp and ID columns. By using bq query – -dry_run you learn that the query triggers a full scan of the table, even though the filter on timestamp and ID select a tiny fraction of the overall data. You want to reduce the amount of data scanned by BigQuery with minimal changes to existing SQL queries. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a separate table for each ID.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use the LIMIT keyword to reduce the number of rows returned.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Recreate the table with a partitioning column and clustering column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the bq query - -maximum_bytes_billed flag to restrict the number of bytes billed.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are planning to migrate your current on-premises Apache Hadoop deployment to the cloud. You need to ensure that the deployment is as fault-tolerant and cost-effective as possible for long-running batch jobs. You want to use a managed service. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Deploy a Cloud Dataproc cluster. Use a standard persistent disk and 50% preemptible worker</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Deploy a Cloud Dataproc cluste</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use an SSD persistent disk and 50% preemptible worker</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install Hadoop and Spark on a 10-node Compute Engine instance group with standard instance</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install the Cloud Storage connector, and store the data in Cloud Storage. Change references in scripts from hdfs:// to gs://</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install Hadoop and Spark on a 10-node Compute Engine instance group with preemptible instances.Store data in HDF. Change references in scripts from hdfs:// to gs://</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You want to migrate an on-premises Hadoop system to Cloud Dataproc. Hive is the primary tool in use, and the data format is Optimized Row Columnar (ORC). All ORC files have been successfully copied to a Cloud Storage bucket. You need to replicate some data to the cluster’s local Hadoop Distributed File System (HDFS) to maximize performance. What are two ways to start using Hive in Cloud Dataproc? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDF. Mount the Hive tables locally.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluster. Mount the Hive tables locally.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluste. Then run the Hadoop utility to copy them do HDF. Mount the Hive tables from HDFS.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Leverage Cloud Storage connector for Hadoop to mount the ORC files as external Hive table. Replicate external Hive tables to the native ones.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load the ORC files into BigQuery. Leverage BigQuery connector for Hadoop to mount the BigQuery tables as external Hive table. Replicate external Hive tables to the native ones.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have a petabyte of analytics data and need to design a storage and processing platform for it. You must be able to perform data warehouse-style analytics on the data in Google Cloud and expose the dataset as files for batch analysis tools in other cloud providers. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Store and process the entire dataset in BigQuery.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store and process the entire dataset in Cloud Bigtable.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store the full dataset in BigQuery, and store a compressed copy of the data in a Cloud Storage bucket.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Store the warm data as files in Cloud Storage, and store the active data in BigQuer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep this ratio as 80% warm and 20% active.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You store historic data in Cloud Storage. You need to perform analytics on the historic data. You want to use a solution to detect invalid data entries and perform data transformations that will not require programming or knowledge of SQL. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Use Cloud Dataflow with Beam to detect errors and perform transformations.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataprep with recipes to detect errors and perform transformations.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataproc with a Hadoop job to detect errors and perform transformations.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use federated tables in BigQuery with queries to detect errors and perform transformations.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You want to use a database of information about tissue samples to classify future tissue samples as either normal or mutated. You are evaluating an unsupervised anomaly detection method for classifying the tissue samples. Which two characteristic support this method? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>There are very few occurrences of mutations relative to normal samples.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>There are roughly equal occurrences of both normal and mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You expect future mutations to have different features from the mutated samples in the database.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>You expect future mutations to have similar features to the mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You already have labels for which samples are mutated and which are normal in the database.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have spent a few days loading data from comma-separated values (CSV) files into the Google BigQuery table CLICK_STREAM. The column DT stores the epoch time of click events. For convenience, you chose a simple schema where every field is treated as the STRING type. Now, you want to compute web session durations of users who visit your site, and you want to change its data type to the TIMESTAMP. You want to minimize the migration effort without making future queries computationally expensive. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Delete the table CLICK_STREAM, and then re-create it such that the column DT is of the TIMESTAMP type. </detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reload the data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a column TS of the TIMESTAMP type to the table CLICK_STREAM, and populate the numeric values from the column TS for each ro</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reference the column TS instead of the column DT from now on.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a view CLICK_STREAM_V, where strings from the column DT are cast into TIMESTAMP value</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reference the view CLICK_STREAM_V instead of the table CLICK_STREAM from now on.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add two columns to the table CLICK STREAM: TS of the TIMESTAMP type and IS_NEW of the BOOLEAN typ</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You work for a car manufacturer and have set up a data pipeline using Google Cloud Pub/Sub to capture anomalous sensor events. You are using a push subscription in Cloud Pub/Sub that calls a custom HTTPS endpoint that you have created to take action of these anomalous events as they occur. Your custom HTTPS endpoint keeps getting an inordinate amount of duplicate messages. What is the most likely cause of these duplicate messages?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The message body for the sensor event is too large.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Your custom endpoint has an out-of-date SSL certificate.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The Cloud Pub/Sub topic has too many messages published to it.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Your custom endpoint is not acknowledging messages within the acknowledgement deadline.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your company is performing data preprocessing for a learning algorithm in Google Cloud Dataflow. Numerous data logs are being are being generated during this step, and the team wants to analyze them. Due to the dynamic nature of the campaign, the data is growing exponentially every hour.

The data scientists have written the following code to read the data for a new key features in the logs. 

BigQueryIO.Read
.named(“ReadLogData”)
.from(“clouddataflow-readonly:samples.log_data”)

You want to improve the performance of this data read. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Specify the TableReference object in the code.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use .fromQuery operation to read specific fields from the table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use of both the Google BigQuery TableSchema and TableFieldSchema classes.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Call a transform that returns TableRow objects, where each element in the PCollexction represents a single row in the table.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your company uses a proprietary system to send inventory data every 6 hours to a data ingestion service in the cloud. Transmitted data includes a payload of several fields and the timestamp of the transmission. If there are any concerns about a transmission, the system re-transmits the data. How should you deduplicate the data most efficiency?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Assign global unique identifiers (GUID) to each data entry.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Compute the hash value of each data entry, and compare it with all historical data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store each data entry as the primary key in a separate database and apply an index.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Maintain a database table to store the hash value and other metadata for each data entry.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your company built a TensorFlow neural-network model with a large number of neurons and layers. The model fits well for the training data. However, when tested against new data, it performs poorly. What method can you employ to address this?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Threading</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Serialization</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Dropout Methods</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dimensionality Reduction</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are designing the database schema for a machine learning-based food ordering service that will predict what users want to eat. Here is some of the information you need to store:

The user profile: What the user likes and doesn’t like to eat
The user account information: Name, address, preferred meal times
The order information: When orders are made, from where, to whom
The database will be used to store all the transactional data of the product. 

You want to optimize the data schema. Which Google Cloud Platform product should you use?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud SQL</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Bigtable</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Datastore</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which of these is NOT a way to customize the software on Dataproc cluster instances?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Set initialization actions</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Modify configuration files using cluster properties</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Configure the cluster using Cloud Deployment Manager</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Log into the master node and make changes from there</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which action can a Cloud Dataproc Viewer perform?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Submit a job.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a cluster.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Delete a cluster.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>List the jobs.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which Java SDK class can you use to run your Dataflow programs locally?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>LocalRunner</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>DirectPipelineRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>MachineRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>LocalPipelineRunner</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>What is the general recommendation when designing your row keys for a Cloud Bigtable schema?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Include multiple time series values within the row key</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep the row keep as an 8 bit integer</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Keep your row key reasonably short</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep your row key as long as the field permits</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Suppose you have a dataset of images that are each labeled as to whether or not they contain a human face. To create a neural network that recognizes human faces in images using this labeled dataset, what approach would likely be the most effective?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use K-means Clustering to detect faces in the pixels.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use feature engineering to add features for eyes, noses, and mouths to the input data.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use deep learning by creating a neural network with multiple hidden layers to automatically detect features of faces.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Build a neural network with an input layer of pixels, a hidden layer, and an output layer with two categories.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You want to create a machine learning model that predicts the price of a particular stock based on its recent price history, what type of estimator should you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Unsupervised learning</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Regressor</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Classifier</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Clustering estimator</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Which of these operations can you perform from the BigQuery Web UI?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Upload a file in SQL format.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Load data with nested and repeated fields.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload a 20 MB file.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload multiple files using a wildcard.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>When using Cloud Dataproc clusters, you can access the YARN web interface by configuring a browser to connect through a proxy.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>HTTPS</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>VPN</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>SOCKS</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>HTTP</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You decided to use Cloud Datastore to ingest vehicle telemetry data in real time. You want to build a storage system that will account for the long-term data growth, while keeping the costs low. You also want to create snapshots of the data periodically, so that you can make a point-in-time (PIT) recovery, or clone a copy of the data for Cloud Datastore in a different environment. You want to archive these snapshots for a long time. Which two methods can accomplish this? Choose 2 answers.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use managed export, and store the data in a Cloud Storage bucket using Nearline or Coldline class.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use managed export, and then import to Cloud Datastore in a separate project under a unique namespace reserved for that export.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use managed export, and then import the data into a BigQuery table created just for that export, and delete temporary export files.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write an application that uses Cloud Datastore client libraries to read all the entitie</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Treat each entity as a BigQuery table row via BigQuery streaming inser</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Assign an export timestamp for each export, and attach it as an extra column for each ro</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Make sure that the BigQuery table is partitioned using the export timestamp column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write an application that uses Cloud Datastore client libraries to read all the entitie</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Format the exported data into a JSON fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Apply compression before storing the data in Cloud Source Repositories.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>online retailer has built their current application on Google App Engine. A new initiative at the company mandates that they extend their application to allow their customers to transact directly via the application. They need to manage their shopping transactions and analyze combined data from multiple datasets using a business intelligence (BI) tool. They want to use only a single database for this purpose. Which Google Cloud database should they choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud SQL</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Cloud BigTable</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Datastore</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You need to copy millions of sensitive patient records from a relational database to BigQuery. The total size of the database is 10 TB. You need to design a solution that is secure and time-efficient. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Export the records from the database as an Avro fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload the file to GCS using gsutil, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the records from the database as an Avro fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Copy the file onto a Transfer Appliance and send it to Google, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console. E. Export the records from the database into a CSV fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a public URL for the CSV file, and then use Storage Transfer Service to move the file to Cloud Storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load the CSV file into BigQuery using the BigQuery web UI in the GCP Console.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the records from the database as an Avro fil</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a public URL for the Avro file, and thenuse Storage Transfer Service to move the file to Cloud Storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are building a teal-lime prediction engine that streams files, which may contain Pll (personal identifiable information) data, into Cloud Storage and eventually into BigQuery You want to ensure that the sensitive data is masked but still maintains referential Integrity, because names and emails are often used as join keys How should you use the Cloud Data Loss Prevention API (DLP API) to ensure that the Pll data is not accessible by unauthorized individuals?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Create a pseudonym by replacing the Pll data with cryptogenic tokens, and store the non-tokenized data in a locked-down button.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Redact all Pll data, and store a version of the unredacted data in a locked-down bucket</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Scan every table in BigQuery, and mask the data it finds that has Pll</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a pseudonym by replacing Pll data with a cryptographic format-preserving token</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You work for a shipping company that uses handheld scanners to read shipping labels. Your company has strict data privacy standards that require scanners to only transmit recipients’ personally identifiable information (PII) to analytics systems, which violates user privacy rules. You want to quickly build a scalable solution using cloud-native managed services to prevent exposure of PII to the analytics systems. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create an authorized view in BigQuery to restrict access to tables with sensitive data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install a third-party data validation tool on Compute Engine virtual machines to check the incoming datafor sensitive information.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Stackdriver logging to analyze the data passed through the total pipeline to identify transactions that may contain sensitive information.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Build a Cloud Function that reads the topics and makes a call to the Cloud Data Loss Prevention AP</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the tagging and confidence levels to either pass or quarantine the data in a bucket for review.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have historical data covering the last three years in BigQuery and a data pipeline that delivers new data to BigQuery daily. You have noticed that when the Data Science team runs a query filtered on a date column and limited to 30–90 days of data, the query scans the entire table. You also noticed that your bill is increasing more quickly than you expected. You want to resolve the issue as cost-effectively as possible while maintaining the ability to conduct SQL queries. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Re-create the tables using DD</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Partition the tables by a column containing a TIMESTAMP or DATEType.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Recommend that the Data Science team export the table to a CSV file on Cloud Storage and use Cloud Datalab to explore the data by reading the files directly.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Modify your pipeline to maintain the last 30–90 days of data in one table and the longer history in a different table to minimize full table scans over the entire history.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write an Apache Beam pipeline that creates a BigQuery table per da</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Recommend that the Data Science team use wildcards on the table name suffixes to select the data they need.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are a head of BI at a large enterprise company with multiple business units that each have different priorities and budgets. You use on-demand pricing for BigQuery with a quota of 2K concurrent on-demand slots per project. Users at your organization sometimes don’t get slots to execute their query and you need to correct this. You’d like to avoid introducing new projects to your account. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Convert your batch BQ queries into interactive BQ queries.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create an additional project to overcome the 2K on-demand per-project quota.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Switch to flat-rate pricing and establish a hierarchical priority model for your projects.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the amount of concurrent slots per project at the Quotas page at the Cloud Console.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>aerospace company uses a proprietary data format to store its night data. You need to connect this new data source to BigQuery and stream the data into BigQuery. You want to efficiency import the data into BigQuery where consuming as few resources as possible. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use a standard Dataflow pipeline to store the raw data in BigQuery and then transform the format later when the data is used.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write a shell script that triggers a Cloud Function that performs periodic ETL batch jobs on the new data source</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Apache Hive to write a Dataproc job that streams the data into BigQuery in CSV format</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use an Apache Beam custom connector to write a Dataflow pipeline that streams the data into BigQuery in Avro format</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>A live TV show asks viewers to cast votes using their mobile phones. The event generates a large volume of data during a 3 minute period. You are in charge of the Voting restructure* and must ensure that the platform can handle the load and Hal all votes are processed. You must display partial results write voting is open. After voting doses you need to count the votes exactly once white optimizing cost. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a Memorystore instance with a high availability (HA) configuration</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Write votes to a Pub Sub tope and have Cloud Functions subscribe to it and write voles to BigQuery</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Write votes to a Pub/Sub tope and toad into both Bigtable and BigQuery via a Dataflow pipeline Query Bigtable for real-time results and BigQuery for later analysis Shutdown the Bigtable instance when voting concludesD Create a Cloud SQL for PostgreSQL database with high availability (HA) configuration and multiple read replicas</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are selecting services to write and transform JSON messages from Cloud Pub/Sub to BigQuery for a data pipeline on Google Cloud. You want to minimize service costs. You also want to monitor and accommodate input data volume that will vary in size with minimal manual intervention. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataproc to run your transformation. Monitor CPU utilization for the cluster. Resize the number of worker nodes in your cluster via the command line.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use Cloud Dataproc to run your transformation. Use the diagnose command to generate an operational output archive. Locate the bottleneck and adjust cluster resources.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to run your transformation. Monitor the job system lag with Stackdriver.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the default autoscaling setting for worker instances.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to run your transformation. Monitor the total execution time for a sampling of job. Configure the job to use non-default Compute Engine machine types when needed.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have a requirement to insert minute-resolution data from 50,000 sensors into a BigQuery table. You expect significant growth in data volume and need the data to be available within 1 minute of ingestion for real-time analysis of aggregated trends. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use bq load to load a batch of sensor data every 60 seconds.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a Cloud Dataflow pipeline to stream data into the BigQuery table.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use the INSERT statement to insert a batch of data every 60 seconds.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the MERGE statement to apply updates in batch every 60 seconds.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are migrating an application that tracks library books and information about each book, such as author or year published, from an on-premises data warehouse to BigQuery In your current relational database, the author information is kept in a separate table and joined to the book information on a common key Based on Google's recommended practice for schema design, how would you structure the data to ensure optimal speed of queries about the author of each book that has been borrowed?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Keep the schema the same, maintain the different tables for the book and each of the attributes, and query as you are doing today</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a table that is wide and includes a column for each attribute, including the author's first name, last name, date of birth, etc</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Create a table that includes information about the books and authors, but nest the author fields inside the author column</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep the schema the same, create a view that joins all of the tables, and always query the view</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your neural network model is taking days to train. You want to increase the training speed. What can you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Subsample your test dataset.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Subsample your training dataset.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the number of input features to your model.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Increase the number of layers in your neural network.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You set up a streaming data insert into a Redis cluster via a Kafka cluster. Both clusters are running on Compute Engine instances. You need to encrypt data at rest with encryption keys that you can create, rotate, and destroy as needed. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a dedicated service account, and use encryption at rest to reference your data stored in your Compute Engine cluster instances as part of your API service calls.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create encryption keys in Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Create encryption keys locally. Upload your encryption keys to Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create encryption keys in Cloud Key Management Service. Reference those keys in your API service calls when accessing the data in your Compute Engine cluster instances.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You want to migrate an on-premises Hadoop system to Cloud Dataproc. Hive is the primary tool in use, and the data format is Optimized Row Columnar (ORC). All ORC files have been successfully copied to a Cloud Storage bucket. You need to replicate some data to the cluster’s local Hadoop Distributed File System (HDFS) to maximize performance. What are two ways to start using Hive in Cloud Dataproc? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDF. Mount the Hive tables locally.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluster. Mount the Hive tables locally.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluste Then run the Hadoop utility to copy them do HDF. Mount the Hive tables from HDFS.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Leverage Cloud Storage connector for Hadoop to mount the ORC files as external Hive table. Replicate external Hive tables to the native ones.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load the ORC files into BigQuery. Leverage BigQuery connector for Hadoop to mount the BigQuery tables as external Hive table. Replicate external Hive tables to the native ones.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are implementing security best practices on your data pipeline. Currently, you are manually executing jobs as the Project Owner. You want to automate these jobs by taking nightly batch files containing non-public information from Google Cloud Storage, processing them with a Spark Scala job on a Google Cloud Dataproc cluster, and depositing the results into Google BigQuery. How should you securely run this workload?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Restrict the Google Cloud Storage bucket so only you can see the files</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Grant the Project Owner role to a service account, and run the job with it</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a service account with the ability to read the batch files and to write to BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a user account with the Project Viewer role on the Cloud Dataproc cluster to read the batch files and write to BigQuery</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to implement a change that would improve query performance in BigQuery. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Implement clustering in BigQuery on the ingest date column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Implement clustering in BigQuery on the package-tracking ID column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Tier older data onto Cloud Storage files, and leverage extended tables.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Re-create the table using data partitioning on the package delivery date.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You need to choose a database to store time series CPU and memory usage for millions of computers. You need to store this data in one-second interval samples. Analysts will be performing real-time, ad hoc analytics against the database. You want to avoid being charged for every query executed and ensure that the schema design will allow for future growth of the dataset. Which database and data model should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a table in BigQuery, and append the new samples for CPU and memory to the table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a wide table in BigQuery, create a column for the sample value at each second, and update the row with the interval for each second</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Create a narrow table in Cloud Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a wide table in Cloud Bigtable with a row key that combines the computer identifier with the sample time at each minute, and combine the values for each second as column data.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are designing storage for two relational tables that are part of a 10-TB database on Google Cloud. You want to support transactions that scale horizontally. You also want to optimize data for range queries on nonkey columns. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud SQL for storage. Add secondary indexes to support query patterns.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to transform data to support query patterns.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Spanner for storage. Add secondary indexes to support query patterns.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to transform data to support query patterns.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are migrating your data warehouse to Google Cloud and decommissioning your on-premises data center Because this is a priority for your company, you know that bandwidth will be made available for the initial data load to the cloud. The files being transferred are not large in number, but each file is 90 GB Additionally, you want your transactional systems to continually update the warehouse on Google Cloud in real time What tools should you use to migrate the data and ensure that it continues to write to your warehouse?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Storage Transfer Service for the migration, Pub/Sub and Cloud Data Fusion for the real-time updates</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>BigQuery Data Transfer Service for the migration, Pub/Sub and Dataproc for the real-time updates</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>gsutil for the migration; Pub/Sub and Dataflow for the real-time updates</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>gsutil for both the migration and the real-time updates</detail>
        </option>
      </options>
    </answer>
  </entry>
</bank>