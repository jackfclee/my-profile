<?xml version="1.0" encoding="UTF-8"?>
<bank>
  <topic>PDE17 - Brain Dump (google.passleader.professional-data-engineer.brain.dumps.2021-jul-04.by.fitch.56q.vce)</topic>
  <!-- STRUCTURE DEFINITION:
  </entry>
  <entry>
    <question>XXX</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>XXX</detail>
        </option>
      </options>
    </answer>
  </entry>
  -->

  <entry>
    <question>You want to use a database of information about tissue samples to classify future tissue samples as either normal or mutated. You are evaluating an unsupervised anomaly detection method for classifying the tissue samples. Which two characteristic support this method? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>There are very few occurrences of mutations relative to normal samples.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>There are roughly equal occurrences of both normal and mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You expect future mutations to have different features from the mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You expect future mutations to have similar features to the mutated samples in the database.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>You already have labels for which samples are mutated and which are normal in the database.</detail>
        </option>
      </options>
    </answer>Answer:  BC
  </entry>

  <entry>
    <question>Your company is running their first dynamic campaign, serving different offers by analyzing real-time data during the holiday season. The data scientists are collecting terabytes of data that rapidly grows every hour during their 30-day campaign. They are using Google Cloud Dataflow to preprocess the data and collect the feature (signals) data that is needed for the machine learning model in Google Cloud Bigtable. The team is observing suboptimal performance with reads and writes of their initial load of 10 TB of data. They want to improve this performance while minimizing cost. What should they do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Redefine the schema by evenly distributing reads and writes across the row space of the table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The performance issue should be resolved over time as the site of the BigDate cluster is increased.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Redesign the schema to use a single row key to identify values that need to be updated frequently in the cluster.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Redesign the schema to use row keys based on numeric IDs that increase sequentially per user viewing the offers.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>

  <entry>
    <question>Flowlogisticâ€™s management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system. You need to build a new system on Google Cloud Platform (GCP) that will feed the proprietary tracking software. The system must be able to ingest data from a variety of global sources, process and query in real-time, and store the data reliably. Which combination of GCP products should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Pub/Sub, Cloud Dataflow, and Local SSD</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Pub/Sub, Cloud SQL, and Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Load Balancing, Cloud Dataflow, and Cloud Storage</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>You create a new report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. It is company policy to ensure employees can view only the data associated with their region, so you create and populate a table for each region. You need to enforce the regional access policy to the data.
Which two actions should you take? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Ensure all the tables are included in global dataset.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Ensure each table is included in a dataset for a region.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Adjust the settings for each table to allow a related region-based security group view access.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Adjust the settings for each view to allow a related region-based security group view access.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Adjust the settings for each dataset to allow a related region-based security group view access.</detail>
        </option>
      </options>
    </answer>Answer:  BD
  </entry>


  <entry>
    <question>All Google Cloud Bigtable client requests go through a front-end server they are sent to a Cloud Bigtable node.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>before</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>after</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>only if</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>once</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>Which of these is NOT a way to customize the software on Dataproc cluster instances?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Set initialization actions</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Modify configuration files using cluster properties</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Configure the cluster using Cloud Deployment Manager</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Log into the master node and make changes from there</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>Which of these operations can you perform from the BigQuery Web UI?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Upload a file in SQL format.
 two.)
</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load data with nested and repeated fields.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload a 20 MB file.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload multiple files using a wildcard.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>What are two methods that can be used to denormalize tables in BigQuery?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>1) Split table into multiple tables; 2) Use a partitioned table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>1) Join tables into one table; 2) Use nested repeated fields</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>1) Use a partitioned table; 2) Join tables into one table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>1) Use nested repeated fields; 2) Use a partitioned table</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>Which role must be assigned to a service account used by the virtual machines in a Dataproc cluster so they can execute jobs?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Dataproc Worker</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataproc Viewer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataproc Runner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataproc Editor</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>Which Java SDK class can you use to run your Dataflow programs locally?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>LocalRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>DirectPipelineRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>MachineRunner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>LocalPipelineRunner</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>Which is not a valid reason for poor Cloud Bigtable performance?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The workload isn't appropriate for Cloud Bigtable.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The table's schema is not designed correctly.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The Cloud Bigtable cluster has too many nodes.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>There are issues with the network connection.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>Scaling a Cloud Dataproc cluster typically involves .</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>increasing or decreasing the number of worker nodes</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>increasing or decreasing the number of master nodes</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>moving memory to run more applications on a single node</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>deleting applications from unused nodes periodically</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>To give a user read permission for only the first three columns of a table, which access control method would you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Primitive role</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Predefined role</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Authorized view</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>It's not possible to give access to only the first three columns of a table.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>

  <entry>
    <question>You want to use a BigQuery table as a data sink. In which writing mode(s) can you use BigQuery as a sink?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Both batch and streaming</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>BigQuery cannot be used as a sink</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Only batch</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Only streaming</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
  <entry>
    <question>
Cloud Dataproc is a managed Apache Hadoop and Apache service.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Blaze</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Spark</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Fire</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Ignite</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>1

Which action can a Cloud Dataproc Viewer perform?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Submit a job.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a cluster.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Delete a cluster.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>List the jobs.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>


  <entry>
    <question>9

You are running a pipeline in Cloud Dataflow that receives messages from a Cloud Pub/Sub topic and writes the results to a BigQuery dataset in the EU. Currently, your pipeline is located in europe-west4 and has a maximum of 3 workers, instance type n1-standard-1. You notice that during peak periods, your pipeline is struggling to process records in a timely fashion, when all 3 workers are at maximum CPU utilization. Which two actions can you take to increase performance of your pipeline? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Increase the number of max workers</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a larger instance type for your Cloud Dataflow workers</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Change the zone of your Cloud Dataflow pipeline to run in us-central1</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a temporary table in Cloud Bigtable that will act as a buffer for new dat</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Cloud Bigtable to BigQuery F. Create a temporary table in Cloud Spanner that will act as a buffer for new dat</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Cloud Spanner to BigQuery</detail>
        </option>
      </options>
    </answer>Answer:  BE
  </entry>
  <entry>
    <question>9

A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to implement a change that would improve query performance in BigQuery. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Implement clustering in BigQuery on the ingest date column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Implement clustering in BigQuery on the package-tracking ID column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Tier older data onto Cloud Storage files, and leverage extended tables.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Re-create the table using data partitioning on the package delivery date.</detail>
        </option>
      </options>
    </answer>Answer:  A

   </entry>
  <entry>
    <question>4


You have a requirement to insert minute-resolution data from 50,000 sensors into a BigQuery table. You expect significant growth in data volume and need the data to be available within 1 minute of ingestion for real-time analysis of aggregated trends. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use bq load to load a batch of sensor data every 60 seconds.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a Cloud Dataflow pipeline to stream data into the BigQuery table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the INSERT statement to insert a batch of data every 60 seconds.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the MERGE statement to apply updates in batch every 60 seconds.</detail>
        </option>
      </options>
    </answer>Answer:  C
  </entry>
  <entry>
    <question>6

Youâ€™re training a model to predict housing prices based on an available dataset with real estate properties. Your plan is to train a fully connected neural net, and youâ€™ve discovered that the dataset contains latitude and longtitude of the property. Real estate professionals have told you that the location of the property is highly influential on price, so youâ€™d like to engineer a feature that incorporates this physical dependency.
What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Provide latitude and longtitude as input vectors to your neural net.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a numeric column from a feature cross of latitude and longtitude.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a feature cross of latitude and longtitude, bucketize at the minute level and use L1 regularization during optimization.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a feature cross of latitude and longtitude, bucketize it at the minute level and use L2 regularization during optimization.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>

  <entry>
    <question>7

You are designing storage for two relational tables that are part of a 10-TB database on Google Cloud. You want to support transactions that scale horizontally. You also want to optimize data for range queries on nonkey columns. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud SQL for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add secondary indexes to support query patterns.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud SQL for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to transform data to support query patterns. E. Use Cloud Spanner for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add secondary indexes to support query patterns.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Spanner for storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow to transform data to support query patterns.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>

  <entry>
    <question>1

You have a data pipeline that writes data to Cloud Bigtable using well-designed row keys. You want to monitor your pipeline to determine when to increase the size of you Cloud Bigtable cluster. Which two actions can you take to accomplish this? Choose 2 answers.</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Review Key Visualizer metric</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the size of the Cloud Bigtable cluster when the Read pressure index is above 100.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Review Key Visualizer metric</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the size of the Cloud Bigtable cluster when the Write pressure index is above 100.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Monitor the latency of write operation</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the size of the Cloud Bigtable cluster when there is a sustained increase in write latency.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Monitor storage utilizatio</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the size of the Cloud Bigtable cluster when utilization increases above 70% of max capacity. I. Monitor latency of read operation</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the size of the Cloud Bigtable cluster of read operations take longer than 100 ms.</detail>
        </option>
      </options>
    </answer>Answer:  AC
  </entry>
  <entry>
    <question>6

Government regulations in your industry mandate that you have to maintain an auditable record of access to certain types of datA. Assuming that all expiring logs will be archived correctly, where should you store data that is subject to that mandate?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Encrypted on Cloud Storage with user-supplied encryption key</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>A separate decryption key will be given to each authorized user.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>In Cloud SQL, with separate database user names to each use</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The Cloud SQL Admin activity logs will be used to provide the auditability.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>In a bucket on Cloud Storage that is accessible only by an AppEngine service that collects user information and logs the access before providing a link to the bucket.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>
  <entry>
    <question>2

You have a query that filters a BigQuery table using a WHERE clause on timestamp and ID columns. By using bq query â€“ -dry_run you learn that the query triggers a full scan of the table, even though the filter on timestamp and ID select a tiny fraction of the overall data. You want to reduce the amount of data scanned by BigQuery with minimal changes to existing SQL queries. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a separate table for each ID.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the LIMIT keyword to reduce the number of rows returned.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Recreate the table with a partitioning column and clustering column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the bq query - -maximum_bytes_billed flag to restrict the number of bytes billed.</detail>
        </option>
      </options>
    </answer>Answer:  B
  </entry>

  <entry>
    <question>1

You are planning to migrate your current on-premises Apache Hadoop deployment to the cloud. You need to ensure that the deployment is as fault-tolerant and cost-effective as possible for long-running batch jobs. You want to use a managed service. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Deploy a Cloud Dataproc cluste</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a standard persistent disk and 50% preemptible worker</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Deploy a Cloud Dataproc cluste</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use an SSD persistent disk and 50% preemptible worker</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install Hadoop and Spark on a 10-node Compute Engine instance group with standard instance</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install the Cloud Storage connector, and store the data in Cloud Storag</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Change references in scripts from hdfs:// to gs://</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install Hadoop and Spark on a 10-node Compute Engine instance group with preemptible instances.Store data in HDF K. Change references in scripts from hdfs:// to gs://</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>

  <entry>
    <question>4

You want to migrate an on-premises Hadoop system to Cloud Dataproc. Hive is the primary tool in use, and the data format is Optimized Row Columnar (ORC). All ORC files have been successfully copied to a Cloud Storage bucket. You need to replicate some data to the clusterâ€™s local Hadoop Distributed File System
(HDFS) to maximize performance. What are two ways to start using Hive in Cloud Dataproc? (Choose two.)</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDF</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Mount the Hive tables locally.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluste</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Mount the Hive tables locally.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluste F. Then run the Hadoop utility to copy them do HDF</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Mount the Hive tables from HDFS.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Leverage Cloud Storage connector for Hadoop to mount the ORC files as external Hive table</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Replicate external Hive tables to the native ones.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load the ORC files into BigQuer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Leverage BigQuery connector for Hadoop to mount the BigQuery tables as external Hive table
L. Replicate external Hive tables to the native ones.</detail>
        </option>
      </options>
    </answer>Answer:  BC
  </entry>
  <entry>
    <question>9

You have a petabyte of analytics data and need to design a storage and processing platform for it. You must be able to perform data warehouse-style analytics on the data in Google Cloud and expose the dataset as files for batch analysis tools in other cloud providers. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Store and process the entire dataset in BigQuery.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store and process the entire dataset in Cloud Bigtable.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store the full dataset in BigQuery, and store a compressed copy of the data in a Cloud Storage bucket.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store the warm data as files in Cloud Storage, and store the active data in BigQuer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep this ratio as 80% warm and 20% active.</detail>
        </option>
      </options>
    </answer>Answer:  D
  </entry>
  <entry>
    <question>2

You store historic data in Cloud Storage. You need to perform analytics on the historic data. You want to use a solution to detect invalid data entries and perform data transformations that will not require programming or knowledge of SQL.
What should you do?
</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataflow with Beam to detect errors and perform transformations.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataprep with recipes to detect errors and perform transformations.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Dataproc with a Hadoop job to detect errors and perform transformations.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use federated tables in BigQuery with queries to detect errors and perform transformations.</detail>
        </option>
      </options>
    </answer>Answer:  A
  </entry>
</bank>